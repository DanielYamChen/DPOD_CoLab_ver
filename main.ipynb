{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoDjaM0VEaFv",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4UYpF5lEjLq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "# drive.flush_and_unmount()\n",
        "drive.mount(\"/content/gdrive\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKOw7E3hJF_o"
      },
      "outputs": [],
      "source": [
        "#### When 1st time run CoLab ####\n",
        "# !mkdir Dataset\n",
        "# !ls\n",
        "# !cp -a \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/Pkl_Data/.\" \"./Dataset\"\n",
        "# !cp \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/ape.zip\" \"./Dataset\"\n",
        "# !cp \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/benchviseblue.zip\" \"./Dataset\"\n",
        "# !cp \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/cam.zip\" \"./Dataset\"\n",
        "# !cp \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/can.zip\" \"./Dataset\"\n",
        "# !cp \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/cat.zip\" \"./Dataset\"\n",
        "# !cp \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/driller.zip\" \"./Dataset\"\n",
        "# !cp \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/duck.zip\" \"./Dataset\"\n",
        "# !cp \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/eggbox.zip\" \"./Dataset\"\n",
        "# !cp \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/glue.zip\" \"./Dataset\"\n",
        "# !cp \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/holepuncher.zip\" \"./Dataset\"\n",
        "# !cp \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/iron.zip\" \"./Dataset\"\n",
        "# !cp \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/lamp.zip\" \"./Dataset\"\n",
        "# !cp \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/phone.zip\" \"./Dataset\"\n",
        "\n",
        "# !cp \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/blueduck.zip\" \"./Dataset\"\n",
        "# !cp \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/lmduck.zip\" \"./Dataset\"\n",
        "# !cp \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/yellowduck100.zip\" \"./Dataset\"\n",
        "# !cp \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/blueduck100.zip\" \"./Dataset\"\n",
        "# !cp \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/lmblueduck.zip\" \"./Dataset\"\n",
        "# !cp \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/blueduck2500.zip\" \"./Dataset\"\n",
        "\n",
        "# !cp \"/content/gdrive/MyDrive/CoLabWork/CS839_DLCV_Project/DPOD_later/val2017.zip\" \"./\"\n",
        "\n",
        "!rm -r \"/content/Dataset/duck\"\n",
        "\n",
        "# !echo \"A\"| unzip -q \"/content/Dataset/ape.zip\" -d \"/content\"\n",
        "# !echo \"A\"| unzip -q \"/content/Dataset/benchviseblue.zip\" -d \"/content\"\n",
        "# !echo \"A\"| unzip -q \"/content/Dataset/cam.zip\" -d \"/content\"\n",
        "# !echo \"A\"| unzip -q \"/content/Dataset/can.zip\" -d \"/content\"\n",
        "# !echo \"A\"| unzip -q \"/content/Dataset/cat.zip\" -d \"/content\"\n",
        "# !echo \"A\"| unzip -q \"/content/Dataset/driller.zip\" -d \"/content\"\n",
        "# !echo \"A\"| unzip -q \"/content/Dataset/duck.zip\" -d \"/content\"\n",
        "# !echo \"A\"| unzip -q \"/content/Dataset/eggbox.zip\" -d \"/content\"\n",
        "# !echo \"A\"| unzip -q \"/content/Dataset/glue.zip\" -d \"/content\"\n",
        "# !echo \"A\"| unzip -q \"/content/Dataset/holepuncher.zip\" -d \"/content\"\n",
        "# !echo \"A\"| unzip -q \"/content/Dataset/iron.zip\" -d \"/content\"\n",
        "# !echo \"A\"| unzip -q \"/content/Dataset/lamp.zip\" -d \"/content\"\n",
        "# !echo \"A\"| unzip -q \"/content/Dataset/phone.zip\" -d \"/content\"\n",
        "\n",
        "# !echo \"A\"| unzip -q \"/content/Dataset/blueduck.zip\" -d \"/content\"\n",
        "# !echo \"A\"| unzip -q \"/content/Dataset/lmduck.zip\" -d \"/content\"\n",
        "# !echo \"A\"| unzip -q \"/content/Dataset/yellowduck100.zip\" -d \"/content\"\n",
        "!echo \"A\"| unzip -q \"/content/Dataset/blueduck100.zip\" -d \"/content\"\n",
        "# !echo \"A\"| unzip -q \"/content/Dataset/lmblueduck.zip\" -d \"/content\"\n",
        "# !echo \"A\"| unzip -q \"/content/Dataset/blueduck2500.zip\" -d \"/content\"\n",
        "\n",
        "# !echo \"A\"| unzip -q \"/content/val2017.zip\" -d \"/content\"\n",
        "#### When 1st time run CoLab ####\n",
        "\n",
        "\n",
        "# !mv \"/content/Dataset/lmduck\" \"/content/Dataset/duck\"\n",
        "# !mv \"/content/Dataset/yellowduck100\" \"/content/Dataset/duck\"\n",
        "!mv \"/content/Dataset/blueduck100\" \"/content/Dataset/duck\"\n",
        "# !mv \"/content/Dataset/lmblueduck\" \"/content/Dataset/duck\"\n",
        "# !mv \"/content/Dataset/blueduck2500\" \"/content/Dataset/TransferLearning/duck\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -r \"/content/Dataset/blueduck\""
      ],
      "metadata": {
        "id": "rA7PdwOGy4oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7kGW-vMGxED"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/CoLabWork/CS839_DLCV_Project/DPOD_later')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "oPBjw-dpOPNp"
      },
      "outputs": [],
      "source": [
        "###################\n",
        "#### utils.py #####\n",
        "###################\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "neighbors = NearestNeighbors(n_neighbors=1)\n",
        "\n",
        "# Pickle functions to save dictionaries\n",
        "def save_obj(obj, name):\n",
        "    with open(name + '.pkl', 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "# Pickle functions to load dictionaries\n",
        "def load_obj(name):\n",
        "    with open(name + '.pkl', 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "\n",
        "# Util functions to plot grpahs\n",
        "# Plot all images in the array of tensors in one row\n",
        "def visualize(imgs):\n",
        "    for z in range(len(imgs)):\n",
        "        temp = imgs[z]\n",
        "        if (temp.ndim > 3):  # tensor output in the form NCHW\n",
        "            temp = torch.argmax(temp, dim=1).squeeze()\n",
        "        \n",
        "        if (len(temp.shape) >= 3):\n",
        "            plt.figure()\n",
        "            plt.imshow(np.transpose( temp.detach().numpy().squeeze(), (1, 2, 0)))\n",
        "            plt.show()\n",
        "        \n",
        "        else:\n",
        "            plt.figure()\n",
        "            plt.imshow(temp.detach().numpy(), cmap='gray')\n",
        "\n",
        "\n",
        "# Create a bounding box around the object\n",
        "def create_bounding_box(img, pose, pt_cld_data, intrnsc_mtrx, color=(0, 0, 255)):\n",
        "    # 8 corner points of the ptcld data\n",
        "    min_x, min_y, min_z = pt_cld_data.min(axis=0)\n",
        "    max_x, max_y, max_z = pt_cld_data.max(axis=0)\n",
        "    corners_3D = np.array([[max_x, min_y, min_z], [max_x, min_y, max_z],\n",
        "                           [min_x, min_y, max_z], [min_x, min_y, min_z],\n",
        "                           [max_x, max_y, min_z], [max_x, max_y, max_z],\n",
        "                           [min_x, max_y, max_z], [min_x, max_y, min_z]])\n",
        "    print(corners_3D)\n",
        "    \n",
        "    # convert these 8 3D corners to 2D points\n",
        "    ones = np.ones((corners_3D.shape[0], 1))\n",
        "    homogenous_coordinate = np.append(corners_3D, ones, axis=1)\n",
        "\n",
        "    # Perspective Projection to obtain 2D coordinates for masks\n",
        "    homogenous_2D = intrnsc_mtrx @ (pose @ homogenous_coordinate.T)\n",
        "    coord_2D = homogenous_2D[:2, :] / homogenous_2D[2, :]\n",
        "    coord_2D = ((np.floor(coord_2D)).T).astype(int)\n",
        "\n",
        "    # Draw lines between these 8 points\n",
        "    img = cv2.line(img, tuple(coord_2D[0]), tuple(coord_2D[1]), color, 3)\n",
        "    img = cv2.line(img, tuple(coord_2D[0]), tuple(coord_2D[3]), color, 3)\n",
        "    img = cv2.line(img, tuple(coord_2D[0]), tuple(coord_2D[4]), color, 3)\n",
        "    img = cv2.line(img, tuple(coord_2D[1]), tuple(coord_2D[2]), color, 3)\n",
        "    img = cv2.line(img, tuple(coord_2D[1]), tuple(coord_2D[5]), color, 3)\n",
        "    img = cv2.line(img, tuple(coord_2D[2]), tuple(coord_2D[3]), color, 3)\n",
        "    img = cv2.line(img, tuple(coord_2D[2]), tuple(coord_2D[6]), color, 3)\n",
        "    img = cv2.line(img, tuple(coord_2D[3]), tuple(coord_2D[7]), color, 3)\n",
        "    img = cv2.line(img, tuple(coord_2D[4]), tuple(coord_2D[7]), color, 3)\n",
        "    img = cv2.line(img, tuple(coord_2D[4]), tuple(coord_2D[5]), color, 3)\n",
        "    img = cv2.line(img, tuple(coord_2D[5]), tuple(coord_2D[6]), color, 3)\n",
        "    img = cv2.line(img, tuple(coord_2D[6]), tuple(coord_2D[7]), color, 3)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "# Evaluation metric: predicted pose is correct or not w.r.t. true pose based on ADD score\n",
        "def ADD_correct(pt_cld, true_pose, pred_pose, diameter):\n",
        "    # handle nan values in predicted pose\n",
        "    pred_pose[0:3, 0:3][np.isnan(pred_pose[0:3, 0:3])] = 1\n",
        "    pred_pose[:, 3][np.isnan(pred_pose[:, 3])] = 0\n",
        "\n",
        "    target = pt_cld @ true_pose[0:3, 0:3] + np.array([true_pose[0, 3], true_pose[1, 3], true_pose[2, 3]])\n",
        "    output = pt_cld @ pred_pose[0:3, 0:3] + np.array([pred_pose[0, 3], pred_pose[1, 3], pred_pose[2, 3]])\n",
        "    avg_distance = (np.linalg.norm(output - target)) / pt_cld.shape[0]\n",
        "    threshold = diameter * 0.1\n",
        "    if (avg_distance <= threshold):\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "# Evaluation metric: predicted pose is correct or not w.r.t. true pose based on ADD-S score\n",
        "def ADD_S_correct(pt_cld, true_pose, pred_pose, diameter):\n",
        "    # handle nan values in predicted pose\n",
        "    pred_pose[0:3, 0:3][np.isnan(pred_pose[0:3, 0:3])] = 1\n",
        "    pred_pose[:, 3][np.isnan(pred_pose[:, 3])] = 0\n",
        "\n",
        "    target1 = pt_cld @ true_pose[0:3, 0:3] + np.array([true_pose[0, 3], true_pose[1, 3], true_pose[2, 3]])\n",
        "    target2 = pt_cld @ np.array([[1., 0., 0.], [0., -1., 0.], [0., 0., 1.]]) @ true_pose[0:3, 0:3] + np.array([true_pose[0, 3], true_pose[1, 3], true_pose[2, 3]])\n",
        "    output = pt_cld @ pred_pose[0:3, 0:3] + np.array([pred_pose[0, 3], pred_pose[1, 3], pred_pose[2, 3]])\n",
        "    \n",
        "    # neighbors.fit(target)\n",
        "    # distances, _ = neighbors.kneighbors(output)\n",
        "    # avg_distance = np.mean(distances)\n",
        "    # print(avg_distance)\n",
        "    \n",
        "    avg_distance1 = (np.linalg.norm(output - target1)) / pt_cld.shape[0]\n",
        "    avg_distance2 = (np.linalg.norm(output - target2)) / pt_cld.shape[0]\n",
        "    avg_distance = avg_distance1 if avg_distance1 < avg_distance2 else avg_distance2\n",
        "\n",
        "    threshold = diameter * 0.1\n",
        "    if (avg_distance <= threshold):\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKQsODtvYV5V"
      },
      "outputs": [],
      "source": [
        "######################################\n",
        "#### pose_refiner_architecture.py ####\n",
        "######################################\n",
        "\"\"\" Parts of the Deep Learning Based pose refiner model \"\"\"\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "from scipy.spatial.transform import Rotation as R\n",
        "\n",
        "\n",
        "class Pose_Refiner(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Pose_Refiner, self).__init__()\n",
        "        self.feature_extractor_image = nn.Sequential(*list(models.resnet18(pretrained=True,\n",
        "                                                                           progress=True).children())[:9])\n",
        "        self.feature_extractor_rendered = nn.Sequential(*list(models.resnet18(pretrained=True,\n",
        "                                                                              progress=True).children())[:9])\n",
        "        self.fc_xyhead_1 = nn.Linear(512, 253)\n",
        "        self.fc_xyhead_2 = nn.Linear(256, 2)\n",
        "        self.fc_zhead = nn.Sequential(nn.Linear(512, 256),\n",
        "                                      nn.ReLU(),\n",
        "                                      nn.Linear(256, 1))\n",
        "        self.fc_Rhead_1 = nn.Linear(512, 252)\n",
        "        self.fc_Rhead_2 = nn.Linear(256, 4)\n",
        "\n",
        "        self.relu_layer = nn.ReLU()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        # weight initialization\n",
        "        nn.init.constant_(self.fc_xyhead_1.weight, 0.)\n",
        "        nn.init.constant_(self.fc_xyhead_1.bias, 0.)\n",
        "\n",
        "        weights = torch.zeros((2, 256))\n",
        "        weights[0, 253] = torch.tensor(1.)\n",
        "        weights[1, 254] = torch.tensor(1.)\n",
        "        self.fc_xyhead_2.weight = nn.Parameter(weights)\n",
        "        nn.init.constant_(self.fc_xyhead_2.bias, 0.)\n",
        "\n",
        "        nn.init.constant_(self.fc_zhead.weight, 0.)\n",
        "        nn.init.constant_(self.fc_zhead.bias, 0.)\n",
        "\n",
        "        nn.init.constant_(self.fc_Rhead_1.weight, 0.)\n",
        "        nn.init.constant_(self.fc_Rhead_1.bias, 0.)\n",
        "\n",
        "        rand_weights = torch.zeros((4, 256))\n",
        "        rand_weights[0, 252] = torch.tensor(1.)\n",
        "        rand_weights[1, 253] = torch.tensor(1.)\n",
        "        rand_weights[2, 254] = torch.tensor(1.)\n",
        "        rand_weights[3, 255] = torch.tensor(1.)\n",
        "        self.fc_Rhead_2.weight = nn.Parameter(weights)\n",
        "        nn.init.constant_(self.fc_Rhead_2.bias, 0.)\n",
        "\n",
        "    def forward(self, image, rendered, pred_pose, bs=1):\n",
        "        # extracting the feature vector f\n",
        "        f_image = self.feature_extractor_image(image)\n",
        "        f_rendered = self.feature_extractor_rendered(rendered)\n",
        "        f_image = f_image.view(bs, -1)\n",
        "        f_image = self.relu_layer(f_image)\n",
        "        f_rendered = f_image.view(bs, -1)\n",
        "        f_rendered = self.relu_layer(f_rendered)\n",
        "        f = f_image - f_rendered\n",
        "\n",
        "        # Z refinement head\n",
        "        z = self.fc_zhead(f)\n",
        "\n",
        "        # XY refinement head\n",
        "        f_xy1 = self.fc_xyhead_1(f)\n",
        "        f_xy1 = self.relu_layer(f_xy1)\n",
        "        x_pred = np.reshape(pred_pose[:, 0, 3], (bs, -1))\n",
        "        y_pred = np.reshape(pred_pose[:, 1, 3], (bs, -1))\n",
        "        f_xy1 = torch.cat((f_xy1, x_pred.float().cuda()), 1)\n",
        "        f_xy1 = torch.cat((f_xy1, y_pred.float().cuda()), 1)\n",
        "        f_xy1 = torch.cat((f_xy1, z), 1)\n",
        "        xy = self.fc_xyhead_2(f_xy1.cuda())\n",
        "\n",
        "        # Rotation head\n",
        "        f_r1 = self.fc_Rhead_1(f)\n",
        "        f_r1 = self.relu_layer(f_r1)\n",
        "        r = R.from_matrix(pred_pose[:, 0:3, 0:3])\n",
        "        r = r.as_quat()\n",
        "        r = np.reshape(r, (bs, -1))\n",
        "        f_r1 = torch.cat(\n",
        "            (f_r1, torch.from_numpy(r).float().cuda()), 1)\n",
        "        rot = self.fc_Rhead_2(f_r1)\n",
        "\n",
        "        return xy, z, rot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hC9PfE0tsXPT"
      },
      "outputs": [],
      "source": [
        "############################\n",
        "#### pose_refinement.py ####\n",
        "############################\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from scipy.spatial.transform import Rotation as R\n",
        "import time\n",
        "\n",
        "# from dataset_classes import PoseRefinerDataset\n",
        "# from pose_refiner_architecture import Pose_Refiner\n",
        "\n",
        "\n",
        "def fetch_ptcld_data(pt_cld_data_set, classes, labels, bs):\n",
        "    # detch pt cld data for batchsize\n",
        "    pt_cld_data = []\n",
        "    for i in range(bs):\n",
        "        pt_cld = pt_cld_data_set[classes[ labels[i] ] - 1]\n",
        "        pt_cld = pt_cld[ :, 0 : 3]\n",
        "        # obj_dir = root_dir + label[i] + \"/object.xyz\"\n",
        "        # pt_cld = np.loadtxt(obj_dir, skiprows=1, usecols=(0, 1, 2)) ##############\n",
        "        index = np.random.choice(pt_cld.shape[0], 3000, replace=False)\n",
        "        pt_cld_data.append(pt_cld[index, :])\n",
        "    pt_cld_data = np.stack(pt_cld_data, axis=0)\n",
        "    return pt_cld_data\n",
        "\n",
        "\n",
        "## no. of points is always 3000\n",
        "def Matching_loss(pt_cld_rand, true_pose, pred_pose, bs, training=True):\n",
        "\n",
        "    total_loss = torch.tensor([0.])\n",
        "    total_loss.requires_grad = True\n",
        "    for i in range(0, bs):\n",
        "        pt_cld = pt_cld_rand[i, :, :].squeeze()\n",
        "        TP = true_pose[i, :, :].squeeze()\n",
        "        PP = pred_pose[i, :, :].squeeze()\n",
        "        target = torch.tensor(pt_cld) @ TP[0:3, 0:3] + torch.cat(\n",
        "            (TP[0, 3].view(-1, 1), TP[1, 3].view(-1, 1), TP[2, 3].view(-1, 1)), 1)\n",
        "        output = torch.tensor(pt_cld) @ PP[0:3, 0:3] + torch.cat(\n",
        "            (PP[0, 3].view(-1, 1), PP[1, 3].view(-1, 1), PP[2, 3].view(-1, 1)), 1)\n",
        "        loss = (torch.abs(output - target).sum())/3000\n",
        "        if loss < 100:\n",
        "            total_loss = total_loss + loss\n",
        "        else:  # so that loss isn't NaN\n",
        "            total_loss = total_loss + torch.tensor([100.])\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "## train the pose refinement model\n",
        "def train_pose_refinement(root_dir, dataset_dir, model_dir, classes, epochs=5, transferLearning=True):\n",
        "    if(transferLearning):\n",
        "        pt_cld_data_set = load_obj(dataset_dir + \"pt_cld_data_set_transfer\")\n",
        "    else:\n",
        "        pt_cld_data_set = load_obj(dataset_dir + \"pt_cld_data_set\")\n",
        "    \n",
        "    train_data = PoseRefinerDataset(dataset_dir, classes=classes,\n",
        "                                    transform=transforms.Compose([\n",
        "                                        transforms.ToPILImage(mode=None),\n",
        "                                        transforms.Resize(size=(224, 224)),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize([0.485, 0.456, 0.406], [\n",
        "                                                             0.229, 0.224, 0.225]),\n",
        "                                        transforms.ColorJitter(\n",
        "                                            brightness=0, contrast=0, saturation=0, hue=0)\n",
        "                                    ]), transferLearning=True)\n",
        "\n",
        "    pose_refiner = Pose_Refiner()\n",
        "    pose_refiner.cuda()\n",
        "    if (transferLearning):\n",
        "        pose_refiner.load_state_dict(torch.load(model_dir + 'pose_refiner.pt', map_location=torch.device('cpu')))\n",
        "\n",
        "    # freeze resnet\n",
        "    # pose_refiner.feature_extractor[0].weight.requires_grad = False\n",
        "\n",
        "    batch_size = 4\n",
        "    num_workers = 0\n",
        "    valid_size = 0.2\n",
        "    # obtain training indices that will be used for validation\n",
        "    num_train = len(train_data)\n",
        "    indices = list(range(num_train))\n",
        "    np.random.shuffle(indices)\n",
        "    split = int(np.floor(valid_size * num_train))\n",
        "    train_idices, valid_idices = indices[split:], indices[:split]\n",
        "\n",
        "    # define samplers for obtaining training and validation batches\n",
        "    train_sampler = SubsetRandomSampler(train_idices)\n",
        "    valid_sampler = SubsetRandomSampler(valid_idices)\n",
        "\n",
        "    # prepare data loaders (combine dataset and sampler)\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "                                               sampler=train_sampler, num_workers=num_workers)\n",
        "    valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "                                               sampler=valid_sampler, num_workers=num_workers)\n",
        "\n",
        "    optimizer = optim.Adam(pose_refiner.parameters(),\n",
        "                           lr=3e-4, weight_decay=3e-5)\n",
        "\n",
        "    # number of epochs to train the model\n",
        "    n_epochs = epochs\n",
        "    start_time = time.time()\n",
        "    valid_loss_min = np.Inf  # track change in validation loss\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        \n",
        "        epoch_start_time = time.time()\n",
        "        print(\"----- Epoch Number: \", epoch, \"--------\")\n",
        "\n",
        "        # keep track of training and validation loss\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        pose_refiner.train()\n",
        "        train_batch_idx = 0\n",
        "        for label, image, rendered, true_pose, pred_pose in train_loader:\n",
        "            # move tensors to GPU\n",
        "            image, rendered = image.cuda(), rendered.cuda()\n",
        "            # clear the gradients of all optimized variables\n",
        "            optimizer.zero_grad()\n",
        "            # debugging:\n",
        "            if(train_batch_idx % 100 == 0):\n",
        "                print(train_batch_idx, \":\", image.size(), \",\", rendered.size(), \",\", pred_pose.size())\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            xy, z, rot = pose_refiner(image, rendered, pred_pose, batch_size)\n",
        "            # convert rot quarternion to rotational matrix\n",
        "            rot[torch.isnan(rot)] = 1  # take care of NaN and inf values\n",
        "            rot[rot == float(\"Inf\")] = 1\n",
        "            xy[torch.isnan(xy)] == 0\n",
        "            z[torch.isnan(z)] == 0\n",
        "\n",
        "            rot = torch.tensor(\n",
        "                (R.from_quat(rot.detach().cpu().numpy())).as_matrix())\n",
        "            # update predicted pose\n",
        "            pred_pose[:, 0:3, 0:3] = rot\n",
        "            pred_pose[:, 0, 3] = xy[:, 0]\n",
        "            pred_pose[:, 1, 3] = xy[:, 1]\n",
        "            pred_pose[:, 2, 3] = z.squeeze()\n",
        "            # fetch point cloud data\n",
        "            pt_cld = fetch_ptcld_data(pt_cld_data_set, classes, label, batch_size)\n",
        "            # calculate the batch loss\n",
        "            loss = Matching_loss(pt_cld, true_pose, pred_pose, batch_size)\n",
        "            # backward pass: compute gradient of the loss with respect to model parameters\n",
        "            loss.backward()\n",
        "            # perform a single optimization step (parameter update)\n",
        "            optimizer.step()\n",
        "            # update training loss\n",
        "            train_loss += loss.item()\n",
        "            \n",
        "            # show progress\n",
        "            if(train_batch_idx % 100 == 0):\n",
        "                print(str(train_batch_idx) + \"/\" + str(len(train_loader)) + \n",
        "                      \" finished! {:2.2f}\".format(time.time() - start_time), \" sec\")\n",
        "                start_time = time.time()\n",
        "            \n",
        "            train_batch_idx += 1\n",
        "            # debugging\n",
        "            # if(train_idx >= 101):\n",
        "            #     break\n",
        "        \n",
        "        ######################\n",
        "        # validate the model #\n",
        "        ######################\n",
        "        pose_refiner.eval()\n",
        "        valid_batch_idx = 0\n",
        "        start_time = time.time()\n",
        "        img_size_prvs = None\n",
        "        for label, image, rendered, true_pose, pred_pose in valid_loader:\n",
        "            # move tensors to GPU\n",
        "            image, rendered = image.cuda(), rendered.cuda()\n",
        "            # debugging\n",
        "            if(valid_batch_idx != 0 and image.size() != img_size_prvs):\n",
        "                print(valid_batch_idx, \":\", image.size(), \",\", rendered.size(), \",\", pred_pose.size())\n",
        "                continue\n",
        "            \n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            xy, z, rot = pose_refiner(image, rendered, pred_pose, batch_size)\n",
        "            rot[torch.isnan(rot)] = 1  # take care of NaN and inf values\n",
        "            rot[rot == float(\"Inf\")] = 1            \n",
        "            xy[torch.isnan(xy)] == 0\n",
        "            z[torch.isnan(z)] == 0\n",
        "            # convert R quarternion to rotational matrix\n",
        "            rot = torch.tensor(\n",
        "                (R.from_quat(rot.detach().cpu().numpy())).as_matrix())\n",
        "            # update predicted pose\n",
        "            pred_pose[:, 0:3, 0:3] = rot\n",
        "            pred_pose[:, 0, 3] = xy[:, 0]\n",
        "            pred_pose[:, 1, 3] = xy[:, 1]\n",
        "            pred_pose[:, 2, 3] = z.squeeze()\n",
        "            # fetch point cloud data\n",
        "            pt_cld = fetch_ptcld_data(pt_cld_data_set, classes, label, batch_size)\n",
        "            # calculate the batch loss\n",
        "            loss = Matching_loss(pt_cld, true_pose, pred_pose, batch_size)\n",
        "            # update average validation loss\n",
        "            valid_loss += loss.item()\n",
        "            \n",
        "            img_size_prvs = image.size()\n",
        "            # show progress\n",
        "            if(valid_batch_idx % 100 == 0):\n",
        "                print(str(valid_batch_idx) + \"/\" + str(len(valid_loader)) + \n",
        "                      \" finished! {:2.2f}\".format(time.time() - start_time), \" sec\")\n",
        "                start_time = time.time()\n",
        "            \n",
        "            valid_batch_idx += 1\n",
        "\n",
        "        # calculate average losses\n",
        "        train_loss = train_loss/len(train_loader.sampler)\n",
        "        valid_loss = valid_loss/len(valid_loader.sampler)\n",
        "\n",
        "        # print training/validation statistics\n",
        "        print('Epoch: {}, {:2.2f} sec \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "            epoch, time.time() - epoch_start_time, train_loss, valid_loss))\n",
        "\n",
        "        # save model if validation loss has decreased\n",
        "        if valid_loss <= valid_loss_min:\n",
        "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "                valid_loss_min, valid_loss))\n",
        "            if (transferLearning):\n",
        "                torch.save(pose_refiner.state_dict(), model_dir + 'pose_refiner_transfer.pt')\n",
        "            else:\n",
        "                torch.save(pose_refiner.state_dict(), model_dir + 'pose_refiner.pt')\n",
        "                \n",
        "            valid_loss_min = valid_loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6ZivSyF_0j-"
      },
      "outputs": [],
      "source": [
        "##############################\n",
        "#### create_renderings.py ####\n",
        "##############################\n",
        "import os\n",
        "import re\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import unet_model as UNET\n",
        "import matplotlib.image as mpimg\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "# from helper import *\n",
        "# from dataset_classes import LineMODDataset\n",
        "# from create_ground_truth import get_rot_tra\n",
        "\n",
        "## Create renderings\n",
        "def create_rendering(dataset_dir, intrnsc_mtrx, pred_pose, pt_cld_data, label, idx):\n",
        "    \n",
        "    # pred_pose_adr = dataset_dir + label + '/predicted_pose' + '/info_' + str(idx) + \"_test.txt\"\n",
        "    # rgb_values = np.loadtxt(dataset_dir + label + '/object.xyz', skiprows=1, usecols=(6, 7, 8)) #####################\n",
        "    rgb_values = pt_cld_data[ :, 6 : 9]\n",
        "    # print(\"rgb_values: \", rgb_values.shape)\n",
        "    # print((pt_cld_data[ :, 6 : 9]).shape)\n",
        "    # coords_3d = np.loadtxt(dataset_dir + label + '/object.xyz', skiprows=1, usecols=(0, 1, 2)) #####################\n",
        "    coords_3d = pt_cld_data[ :, 0 : 3]\n",
        "    # print(\"coords_3d: \", coords_3d.shape)\n",
        "    # print((pt_cld_data[ :, 0 : 3]).shape)\n",
        "\n",
        "    ones = np.ones((coords_3d.shape[0], 1))\n",
        "    homo_coord = np.append(coords_3d, ones, axis=1)\n",
        "    # rigid_trnsfrm = np.loadtxt(pred_pose_adr) #####################\n",
        "    \n",
        "    # Perspective Projection to obtain 2D coordinates\n",
        "    # homo_2D = intrnsc_mtrx @ (rigid_trnsfrm @ homo_coord.T)\n",
        "    homo_2D = intrnsc_mtrx @ (pred_pose @ homo_coord.T)\n",
        "    homo_2D[2, :][np.where(homo_2D[2, :] == 0)] = 1\n",
        "    coord_2D = homo_2D[:2, :] / homo_2D[2, :]\n",
        "    coord_2D = ((np.floor(coord_2D)).T).astype(int)\n",
        "    \n",
        "    # Create rendered image\n",
        "    rendered_image = np.zeros((480, 640, 3))\n",
        "    x_2d = np.clip(coord_2D[:, 0], 0, 479)\n",
        "    y_2d = np.clip(coord_2D[:, 1], 0, 639)\n",
        "    rendered_image[x_2d, y_2d, :] = rgb_values\n",
        "    temp = np.sum(rendered_image, axis=2)\n",
        "    non_zero_indices = np.argwhere(temp > 0)\n",
        "    min_x = non_zero_indices[:, 0].min()\n",
        "    max_x = non_zero_indices[:, 0].max()\n",
        "    min_y = non_zero_indices[:, 1].min()\n",
        "    max_y = non_zero_indices[:, 1].max()\n",
        "    cropped_rendered_image = rendered_image[min_x : max_x + 1, min_y : max_y + 1, : ]\n",
        "    if (cropped_rendered_image.shape[0] > 240 or cropped_rendered_image.shape[1] > 320):\n",
        "        cropped_rendered_image = cv2.resize(np.float32(\n",
        "            cropped_rendered_image), (320, 240), interpolation=cv2.INTER_AREA)\n",
        "    \n",
        "    return cropped_rendered_image\n",
        "\n",
        "\n",
        "# Create and save the dataset of predicted poses of LineMODDataset\n",
        "def create_pred_pose_dataset(root_dir, dataset_dir, classes, transferLearning):\n",
        "    train_data = LineMODDataset(dataset_dir, classes=classes, \n",
        "                                transform=transforms.Compose([transforms.ToTensor()]),\n",
        "                                transferLearning=True)\n",
        "    regex = re.compile(r'\\d+')\n",
        "    start_time = time.time()\n",
        "    pred_poses_dict = {}\n",
        "    for label in classes:\n",
        "        pred_poses_dict[label] = {}\n",
        "    \n",
        "    for train_img_idx in range(len(train_data)):\n",
        "        \n",
        "        img_adr, _, _, _, _ = train_data[train_img_idx]\n",
        "        label = os.path.split( os.path.split( os.path.dirname(img_adr) )[0] )[1]\n",
        "        idx = regex.findall( os.path.split(img_adr)[1] )[0]\n",
        "        \n",
        "        if (transferLearning):\n",
        "            pred_pose_adr = dataset_dir + label + '/predicted_pose' + '/info_' + str(idx) + \"_transfer.txt\"\n",
        "        else:\n",
        "            pred_pose_adr = dataset_dir + label + '/predicted_pose' + '/info_' + str(idx) + \"_test.txt\"\n",
        "        \n",
        "        pred_pose = np.loadtxt(pred_pose_adr)\n",
        "        pred_poses_dict[label][idx] = pred_pose\n",
        "    \n",
        "    if (transferLearning):\n",
        "        save_obj(pred_poses_dict, root_dir + \"pred_poses_dict_transfer\")\n",
        "    else:\n",
        "        save_obj(pred_poses_dict, root_dir + \"pred_poses_dict\")\n",
        "\n",
        "    return pred_poses_dict\n",
        "\n",
        "\n",
        "## Create and store rendered images as input data to train the Pose-Refinement Network\n",
        "def create_refinement_inputs(root_dir, dataset_dir, model_dir, classes, intrnsc_mtrx, transferLearning):\n",
        "    if (transferLearning):\n",
        "        pred_poses_dict = load_obj(dataset_dir + \"pred_poses_dict_transfer\")\n",
        "        pt_cld_data_set = load_obj(dataset_dir + \"pt_cld_data_set_transfer\")\n",
        "    else:\n",
        "        pred_poses_dict = load_obj(dataset_dir + \"pred_poses_dict\")\n",
        "        pt_cld_data_set = load_obj(dataset_dir + \"pt_cld_data_set\")\n",
        "    \n",
        "    correspondence_block = UNET.UNet(n_channels=3, out_channels_id=14, out_channels_uv=256, bilinear=True)\n",
        "    correspondence_block.cuda()\n",
        "    if(transferLearning):\n",
        "        correspondence_block.load_state_dict(torch.load(model_dir + 'correspondence_block_transfer.pt', map_location=torch.device('cpu')))    \n",
        "    else:\n",
        "        correspondence_block.load_state_dict(torch.load(model_dir + 'correspondence_block.pt', map_location=torch.device('cpu')))\n",
        "    \n",
        "    train_data = LineMODDataset(dataset_dir, classes=classes, \n",
        "                                transform=transforms.Compose([transforms.ToTensor()]),\n",
        "                                transferLearning=True)\n",
        "\n",
        "    upsampled = nn.Upsample(size=[240, 320], mode='bilinear', align_corners=False)\n",
        "\n",
        "    regex = re.compile(r'\\d+')\n",
        "    count = 0\n",
        "    start_time = time.time()\n",
        "    for i in range(len(train_data)):\n",
        "        \n",
        "        img_adr, img, _, _, _ = train_data[i]\n",
        "\n",
        "        label = os.path.split(os.path.split(os.path.dirname(img_adr))[0])[1]\n",
        "        idx = regex.findall(os.path.split(img_adr)[1])[0]\n",
        "        \n",
        "        suffix = \"_transfer.png\" if transferLearning else \"_test.png\"\n",
        "        adr_rendered = dataset_dir + label + \"/pose_refinement/rendered/color\" + str(idx) + suffix\n",
        "        adr_img = dataset_dir + label + \"/pose_refinement/real/color\" + str(idx) + suffix\n",
        "        \n",
        "        ## find the object in the image using the idmask\n",
        "        img = img.view(1, img.shape[0], img.shape[1], img.shape[2])\n",
        "        idmask_pred, _, _ = correspondence_block(img.cuda())\n",
        "        idmask = torch.argmax(idmask_pred, dim=1).squeeze().cpu()\n",
        "        coord_2d = (idmask == classes[label]).nonzero(as_tuple=True)\n",
        "        if (coord_2d[0].nelement() != 0):\n",
        "            \n",
        "            coord_2d = torch.cat((coord_2d[0].view(coord_2d[0].shape[0], 1),\n",
        "                                  coord_2d[1].view(coord_2d[1].shape[0], 1)), 1)\n",
        "            min_x = coord_2d[:, 0].min()\n",
        "            max_x = coord_2d[:, 0].max()\n",
        "            min_y = coord_2d[:, 1].min()\n",
        "            max_y = coord_2d[:, 1].max()\n",
        "            img = img.squeeze().transpose(1, 2).transpose(0, 2)\n",
        "            obj_img = img[min_x:max_x+1, min_y:max_y+1, :]\n",
        "            \n",
        "            ## saving in the correct format using upsampling\n",
        "            obj_img = obj_img.transpose(0, 1).transpose(0, 2).unsqueeze(dim=0)\n",
        "            obj_img = upsampled(obj_img)\n",
        "            obj_img = obj_img.squeeze().transpose(0, 2).transpose(0, 1)\n",
        "            mpimg.imsave(adr_img, obj_img.squeeze().numpy())\n",
        "\n",
        "            # create rendering for an object\n",
        "            # cropped_rendered_image = create_rendering(dataset_dir, intrnsc_mtrx, label, idx)\n",
        "            cropped_rendered_image = create_rendering(dataset_dir, intrnsc_mtrx,\n",
        "                                                      pred_poses_dict[label][idx],\n",
        "                                                      pt_cld_data_set[classes[label] - 1],\n",
        "                                                      label, idx)\n",
        "            rendered_img = torch.from_numpy(cropped_rendered_image)\n",
        "            rendered_img = rendered_img.unsqueeze(dim=0)\n",
        "            rendered_img = rendered_img.transpose(1, 3).transpose(2, 3)\n",
        "            rendered_img = upsampled(rendered_img)\n",
        "            rendered_img = rendered_img.squeeze().transpose(0, 2).transpose(0, 1)\n",
        "            mpimg.imsave(adr_rendered, rendered_img.numpy())\n",
        "\n",
        "        else:  # object not present in idmask prediction\n",
        "            count += 1\n",
        "            mpimg.imsave(adr_rendered, np.zeros((240, 320)))\n",
        "            mpimg.imsave(adr_img, np.zeros((240, 320)))\n",
        "        \n",
        "        ## show the progress\n",
        "        if (i % 100 == 0):\n",
        "            print(str(i) + \"/\" + str(len(train_data)) +\n",
        "                  \" finished! {:2.2f}\".format(time.time() - start_time), \" sec\")\n",
        "            start_time = time.time()\n",
        "    \n",
        "    print(\"Number of outliers: \", count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1lFhpyoWD9y"
      },
      "outputs": [],
      "source": [
        "#######################\n",
        "#### pose_block.py ####\n",
        "#######################\n",
        "import os\n",
        "import re\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import unet_model as UNET\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "# from dataset_classes import LineMODDataset\n",
        "# from helper import load_obj\n",
        "\n",
        "def initial_pose_estimation(dataset_dir, model_dir, classes, intrnsc_mtrx, transferLearning):\n",
        "\n",
        "    # LineMOD Dataset\n",
        "    train_data = LineMODDataset(dataset_dir, classes=classes,\n",
        "                                transform=transforms.Compose([transforms.ToTensor()]),\n",
        "                                transferLearning=transferLearning)\n",
        "\n",
        "    # load the best correspondence block weights\n",
        "    correspondence_block = UNET.UNet(\n",
        "        n_channels=3, out_channels_id=14, out_channels_uv=256, bilinear=True)\n",
        "    correspondence_block.cuda()\n",
        "    if (transferLearning):\n",
        "        correspondence_block.load_state_dict(torch.load(model_dir + 'correspondence_block_transfer.pt', map_location=torch.device('cpu')))\n",
        "    else:\n",
        "        correspondence_block.load_state_dict(torch.load(model_dir + 'correspondence_block.pt', map_location=torch.device('cpu')))\n",
        "\n",
        "    # initial 6D pose prediction\n",
        "    regex = re.compile(r'\\d+')\n",
        "    outliers = 0\n",
        "    start_time = time.time()\n",
        "    for i in range(len(train_data)):\n",
        "        if i % 100 == 0:\n",
        "            print(str(i) + \"/\" + str(len(train_data)) +\n",
        "                  \" finished! {:2.2f}\".format(time.time() - start_time), \" sec\")\n",
        "            start_time = time.time()\n",
        "\n",
        "        img_adr, img, idmask, _, _ = train_data[i]   \n",
        "        label = os.path.split(os.path.split(os.path.dirname(img_adr))[0])[1]\n",
        "        # print(img_adr)\n",
        "        idx = regex.findall(os.path.split(img_adr)[1])[0]\n",
        "        img = img.view(1, img.shape[0], img.shape[1], img.shape[2])\n",
        "        idmask_pred, umask_pred, vmask_pred = correspondence_block(img.cuda())\n",
        "        \n",
        "        # convert the masks to (240, 320) shape\n",
        "        temp = torch.argmax(idmask_pred, dim=1).squeeze().cpu()\n",
        "        upred = torch.argmax(umask_pred, dim=1).squeeze().cpu()\n",
        "        vpred = torch.argmax(vmask_pred, dim=1).squeeze().cpu()\n",
        "        coord_2d = (temp == classes[label]).nonzero(as_tuple=True)\n",
        "        if (transferLearning):\n",
        "            adr = dataset_dir + label + \"/predicted_pose/\" + \"info_\" + str(idx) + \"_transfer.txt\"\n",
        "        else:\n",
        "            adr = dataset_dir + label + \"/predicted_pose/\" + \"info_\" + str(idx) + \"_test.txt\"\n",
        "\n",
        "        coord_2d = torch.cat((coord_2d[0].view(\n",
        "            coord_2d[0].shape[0], 1), coord_2d[1].view(coord_2d[1].shape[0], 1)), 1)\n",
        "        uvalues = upred[coord_2d[:, 0], coord_2d[:, 1]]\n",
        "        vvalues = vpred[coord_2d[:, 0], coord_2d[:, 1]]\n",
        "        dct_keys = torch.cat((uvalues.view(-1, 1), vvalues.view(-1, 1)), 1)\n",
        "        dct_keys = tuple(dct_keys.numpy())\n",
        "        dct = load_obj(dataset_dir + label + \"/UV-XYZ_mapping\") ###########\n",
        "        mapping_2d = []\n",
        "        mapping_3d = []\n",
        "        for count, (u, v) in enumerate(dct_keys):\n",
        "            if (u, v) in dct:\n",
        "                mapping_2d.append(np.array(coord_2d[count]))\n",
        "                mapping_3d.append(dct[(u, v)])\n",
        "        # Get the 6D pose from rotation and translation matrices\n",
        "        # PnP needs atleast 6 unique 2D-3D correspondences to run\n",
        "        if len(mapping_2d) >= 4 or len(mapping_3d) >= 4:\n",
        "            _, rvecs, tvecs, inliers = cv2.solvePnPRansac(np.array(mapping_3d, dtype=np.float32),\n",
        "                                                          np.array(mapping_2d, dtype=np.float32), intrnsc_mtrx, distCoeffs=None,\n",
        "                                                          iterationsCount=150, reprojectionError=1.0, flags=cv2.SOLVEPNP_P3P)\n",
        "            rot, _ = cv2.Rodrigues(rvecs, jacobian=None)\n",
        "            rot[np.isnan(rot)] = 1\n",
        "            tvecs[np.isnan(tvecs)] = 1\n",
        "            tvecs = np.where(-100 < tvecs, tvecs, np.array([-100.]))\n",
        "            tvecs = np.where(tvecs < 100, tvecs, np.array([100.]))\n",
        "            rot_tra = np.append(rot, tvecs, axis=1)\n",
        "            # save the predicted pose\n",
        "            np.savetxt(adr, rot_tra)\n",
        "        else:  # save a pose full of zeros\n",
        "            outliers += 1\n",
        "            rot_tra = np.ones((3, 4))\n",
        "            rot_tra[:, 3] = 0\n",
        "            np.savetxt(adr, rot_tra)\n",
        "    print(\"Number of instances where PnP couldn't be used: \", outliers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cg1EHhiXE9q",
        "cellView": "code"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "############################\n",
        "#### dataset_classes.py ####\n",
        "############################\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "# from create_ground_truth import *\n",
        "# from helper import load_obj\n",
        "# import helper\n",
        "# import pickle\n",
        "\n",
        "class LineMODDataset(Dataset):\n",
        "\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        root_dir (str): path to the dataset\n",
        "        classes (dictionary): values of classes to extract from segmentation mask \n",
        "        transform : Transforms for input image\n",
        "            \"\"\"\n",
        "\n",
        "    def __init__(self, dataset_dir, classes=None, transform=None, transferLearning=False):\n",
        "        self.dataset_dir = dataset_dir\n",
        "        self.transform = transform\n",
        "        self.classes = classes\n",
        "        if (transferLearning):\n",
        "            self.paths_all_images = load_obj(dataset_dir + \"all_images_adr_transfer\")\n",
        "            self.train_img_indices = load_obj(dataset_dir + \"train_images_indices_transfer\")\n",
        "        else:\n",
        "            self.paths_all_images = load_obj(dataset_dir + \"all_images_adr_test\")\n",
        "            self.train_img_indices = load_obj(dataset_dir + \"train_images_indices_test\")\n",
        "        \n",
        "        self.subset_idx = 0\n",
        "        self.suffix = \"_transfer_\" if transferLearning else \"_\"\n",
        "        self.image_subset = load_obj(dataset_dir + \"changed_bgd_set_small\" + self.suffix + str(self.subset_idx))\n",
        "        self.ID_mask_subset = load_obj(dataset_dir + \"ID_mask_set_small\" + self.suffix + str(self.subset_idx))\n",
        "        self.U_mask_subset = load_obj(dataset_dir + \"U_mask_set_small\" + self.suffix + str(self.subset_idx))\n",
        "        self.V_mask_subset = load_obj(dataset_dir + \"V_mask_set_small\" + self.suffix + str(self.subset_idx))\n",
        "        self.subset_size = len(self.image_subset)\n",
        "        print(\"subset size: \", self.subset_size)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.train_img_indices)\n",
        "\n",
        "\n",
        "    def change_subsets(self, subset_idx):\n",
        "        self.subset_idx = subset_idx\n",
        "        self.image_subset = load_obj(self.dataset_dir + \"changed_bgd_set_small\" + self.suffix + str(subset_idx))\n",
        "        self.ID_mask_subset = load_obj(self.dataset_dir + \"ID_mask_set_small\" + self.suffix + str(subset_idx))\n",
        "        self.U_mask_subset = load_obj(self.dataset_dir + \"U_mask_set_small\" + self.suffix + str(subset_idx))\n",
        "        self.V_mask_subset = load_obj(self.dataset_dir + \"V_mask_set_small\" + self.suffix + str(subset_idx))\n",
        "        print(\"subset changed to Subset \", subset_idx)\n",
        "\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "\n",
        "        img_adr = self.paths_all_images[self.train_img_indices[i]]\n",
        "        label = os.path.split(os.path.split(os.path.dirname(img_adr))[0])[1]\n",
        "        regex = re.compile(r'\\d+')\n",
        "        idx = regex.findall(os.path.split(img_adr)[1])[0]\n",
        "        \n",
        "        ## change the group of sub-datasets if index belongs to other subset\n",
        "        if ((i // self.subset_size) != self.subset_idx):\n",
        "            self.change_subsets(i // self.subset_size)\n",
        "        # print(\"subset_size: \", self.subset_size)\n",
        "        # print(\"subset_idx: \", self.subset_idx)\n",
        "        # print(\"i: \", i)\n",
        "        # plt.imshow(self.image_subset[i - self.subset_size * self.subset_idx])\n",
        "        # plt.show()\n",
        "        image = self.image_subset[i - self.subset_size * self.subset_idx]\n",
        "        # print(image.shape)\n",
        "        if (image.shape[2] > 3):\n",
        "            # image = np.concatenate( ( image, np.ones((image.shape[0], image.shape[1], 1)) ), axis=2 )\n",
        "            image = image[:, :, :3]\n",
        "            # print(image.shape)\n",
        "        \n",
        "        ID_mask = self.ID_mask_subset[i - self.subset_size * self.subset_idx]\n",
        "        # if (len(ID_mask.shape) > 2):\n",
        "        #     ID_mask = cv2.cvtColor(ID_mask, cv2.COLOR_BGR2GRAY)\n",
        "        U_mask = self.U_mask_subset[i - self.subset_size * self.subset_idx]\n",
        "        # if (len(U_mask.shape) > 2):\n",
        "        #     U_mask = cv2.cvtColor(U_mask, cv2.COLOR_BGR2GRAY)\n",
        "        V_mask = self.V_mask_subset[i - self.subset_size * self.subset_idx]\n",
        "        # if (len(V_mask.shape) > 2):\n",
        "        #     V_mask = cv2.cvtColor(V_mask, cv2.COLOR_BGR2GRAY)\n",
        "        \n",
        "        \n",
        "\n",
        "        ## old version to read image one by one ##\n",
        "        # if (i % 100 != 0):  # read the image with changed background\n",
        "        #     image = cv2.imread(self.dataset_dir + label + \"/changed_background/color\" + str(idx) + \"_test.png\")\n",
        "        # else:\n",
        "        #     image = cv2.imread(img_adr)\n",
        "\n",
        "        # ID_mask = cv2.imread(self.dataset_dir + label + \"/ground_truth/IDmasks/color\" +\n",
        "        #                     str(idx) + \"_test.png\", cv2.IMREAD_GRAYSCALE)\n",
        "        # U_mask = cv2.imread(self.dataset_dir + label + \"/ground_truth/Umasks/color\" +\n",
        "        #                    str(idx) + \"_test.png\", cv2.IMREAD_GRAYSCALE)\n",
        "        # V_mask = cv2.imread(self.dataset_dir + label + \"/ground_truth/Vmasks/color\" +\n",
        "        #                    str(idx) + \"_test.png\", cv2.IMREAD_GRAYSCALE)\n",
        "        \n",
        "        # if (image is None):\n",
        "        #     print(self.dataset_dir + label + \"/changed_background/color\" + str(idx) + \"_test.png\")\n",
        "        \n",
        "        # if (ID_mask is None):\n",
        "        #     print(self.dataset_dir + label + \"/ground_truth/IDmasks/color\" + str(idx) + \"_test.png\")\n",
        "        \n",
        "        # if (U_mask is None):\n",
        "        #     print(self.dataset_dir + label + \"/ground_truth/Umasks/color\" + str(idx) + \"_test.png\")\n",
        "        \n",
        "        # if (V_mask is None):\n",
        "        #     print(self.dataset_dir + label + \"/ground_truth/Vmasks/color\" + str(idx) + \"_test.png\")\n",
        "\n",
        "        # resize the masks and the image\n",
        "        # image = cv2.resize(image, (image.shape[1]//2, image.shape[0]//2), interpolation=cv2.INTER_AREA)\n",
        "        # ID_mask = cv2.resize(ID_mask, (ID_mask.shape[1]//2, ID_mask.shape[0]//2), interpolation=cv2.INTER_AREA)\n",
        "        # U_mask = cv2.resize(U_mask, (U_mask.shape[1]//2, U_mask.shape[0]//2), interpolation=cv2.INTER_AREA)\n",
        "        # V_mask = cv2.resize(V_mask, (V_mask.shape[1]//2, V_mask.shape[0]//2), interpolation=cv2.INTER_AREA)\n",
        "        # if(i % 50 == 0):\n",
        "        #     print(\"image: \", image.shape)\n",
        "        #     print(\"ID_mask: \", ID_mask.shape)\n",
        "        #     print(\"U_mask: \", U_mask.shape)\n",
        "        #     print(\"V_mask: \", V_mask.shape)\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        ID_mask = (torch.from_numpy(ID_mask)).type(torch.int64)\n",
        "        U_mask = (torch.from_numpy(U_mask)).type(torch.int64)\n",
        "        V_mask = (torch.from_numpy(V_mask)).type(torch.int64)\n",
        "        \n",
        "        return img_adr, image, ID_mask, U_mask, V_mask\n",
        "\n",
        "\n",
        "class PoseRefinerDataset(Dataset):\n",
        "\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        root_dir (str): path to the dataset directory\n",
        "        classes (dict): dictionary containing classes as key  \n",
        "        transform : Transforms for input image\n",
        "            \"\"\"\n",
        "\n",
        "    def __init__(self, dataset_dir, classes=None, transform=None, transferLearning=False):\n",
        "        \n",
        "        self.dataset_dir = dataset_dir\n",
        "        self.transform = transform\n",
        "        self.classes = classes\n",
        "        self.subset_idx = 0\n",
        "        self.transferLearning = transferLearning\n",
        "        if (transferLearning):\n",
        "            self.paths_all_images = load_obj(dataset_dir + \"all_images_adr_transfer\")\n",
        "            self.train_img_indices = load_obj(dataset_dir + \"train_images_indices_transfer\")\n",
        "            self.true_pose_set = load_obj(dataset_dir + \"train_trnsfrm_set_transfer\")\n",
        "            self.pred_poses_dict = load_obj(dataset_dir + \"pred_poses_dict_transfer\")\n",
        "            self.real_subset = load_obj(dataset_dir + \"real_set_transfer_\" + str(self.subset_idx))\n",
        "            self.rendered_subset = load_obj(dataset_dir + \"rendered_set_transfer_\" + str(self.subset_idx))\n",
        "        \n",
        "        else:\n",
        "            self.paths_all_images = load_obj(dataset_dir + \"all_images_adr_test\")\n",
        "            self.train_img_indices = load_obj(dataset_dir + \"train_images_indices_test\")\n",
        "            self.true_pose_set = load_obj(dataset_dir + \"train_trnsfrm_set\")\n",
        "            self.pred_poses_dict = load_obj(dataset_dir + \"pred_poses_dict\")\n",
        "            self.real_subset = load_obj(dataset_dir + \"real_set_\" + str(self.subset_idx))\n",
        "            self.rendered_subset = load_obj(dataset_dir + \"rendered_set_\" + str(self.subset_idx))\n",
        "            \n",
        "        self.subset_size = len(self.real_subset)\n",
        "        print(\"subset size: \", self.subset_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.train_img_indices)\n",
        "\n",
        "    def change_subsets(self, subset_idx):\n",
        "        self.subset_idx = subset_idx\n",
        "        if (self.transferLearning):\n",
        "            self.real_subset = load_obj(dataset_dir + \"real_set_transfer_\" + str(self.subset_idx))\n",
        "            self.rendered_subset = load_obj(dataset_dir + \"rendered_set_transfer_\" + str(self.subset_idx))\n",
        "        else:\n",
        "            self.real_subset = load_obj(dataset_dir + \"real_set_\" + str(self.subset_idx))\n",
        "            self.rendered_subset = load_obj(dataset_dir + \"rendered_set_\" + str(self.subset_idx))\n",
        "        # print(\"subset changed to Subset \", subset_idx)\n",
        "\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        \n",
        "        img_adr = self.paths_all_images[self.train_img_indices[i]]\n",
        "        label = os.path.split(os.path.split(os.path.dirname(img_adr))[0])[1]\n",
        "        regex = re.compile(r'\\d+')\n",
        "        idx = regex.findall(os.path.split(img_adr)[1])[0]\n",
        "        \n",
        "        ## change the group of sub-datasets if index belongs to other subset\n",
        "        if ((i // self.subset_size) != self.subset_idx):\n",
        "            self.change_subsets(i // self.subset_size)\n",
        "        # print(\"subset_size: \", self.subset_size)\n",
        "        # print(\"subset_idx: \", self.subset_idx)\n",
        "        # print(\"i: \", i)\n",
        "        # plt.imshow(self.image_subset[i - self.subset_size * self.subset_idx])\n",
        "        # plt.show()\n",
        "        image = self.real_subset[i - self.subset_size * self.subset_idx]     \n",
        "        rendered = self.rendered_subset[i - self.subset_size * self.subset_idx]\n",
        "        true_pose = self.true_pose_set[i]\n",
        "        pred_pose = self.pred_poses_dict[label][idx]\n",
        "\n",
        "        # image = cv2.imread(self.dataset_dir + label + '/pose_refinement/real/color' + str(idx) + \"_test.png\") #########\n",
        "        # rendered = cv2.imread(self.dataset_dir + label + '/pose_refinement/rendered/color' + str(idx) + \"_test.png\", cv2.IMREAD_GRAYSCALE) #######\n",
        "        # true_pose = get_rot_tra(self.dataset_dir + label + '/data/rot' + str(idx) + \".rot\",\n",
        "        #                         self.dataset_dir + label + '/data/tra' + str(idx) + \".tra\") #########\n",
        "        # pred_pose_adr = self.dataset_dir + label + '/predicted_pose/info_' + str(idx) + \"_test.txt\"\n",
        "        # pred_pose = np.loadtxt(pred_pose_adr) ################\n",
        "        \n",
        "        rendered = cv2.cvtColor(rendered.astype('uint8'), cv2.COLOR_GRAY2RGB)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            rendered = self.transform(rendered)\n",
        "        \n",
        "        return label, image, rendered, true_pose, pred_pose\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chUfRebN3RZi"
      },
      "outputs": [],
      "source": [
        "#################################\n",
        "#### correspondence_block.py ####\n",
        "#################################\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import time\n",
        "\n",
        "# from dataset_classes import LineMODDataset\n",
        "import unet_model as UNET\n",
        "\n",
        "def train_correspondence_block(root_dir, dataset_dir, model_dir, classes, epochs=10, transferLearning=True):\n",
        "\n",
        "    train_data = LineMODDataset(dataset_dir, classes=classes,\n",
        "                                transform=transforms.Compose([transforms.ToTensor(),\n",
        "                                    transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)]\n",
        "                                ), transferLearning=True\n",
        "                               )\n",
        "\n",
        "    batch_size = 4\n",
        "    num_workers = 0\n",
        "    valid_rate = 0.2\n",
        "    # obtain training indices that will be used for validation\n",
        "    num_train = len(train_data)\n",
        "    indices = list(range(num_train))\n",
        "    np.random.shuffle(indices)\n",
        "    split = int(np.floor(valid_rate * num_train))\n",
        "    train_indices, valid_indices = indices[split:], indices[:split]\n",
        "\n",
        "    # define samplers for obtaining training and validation batches\n",
        "    train_sampler = SubsetRandomSampler(train_indices)\n",
        "    valid_sampler = SubsetRandomSampler(valid_indices)\n",
        "\n",
        "    # prepare data loaders (combine dataset and sampler)\n",
        "    train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                               batch_size=batch_size,\n",
        "                                               sampler=train_sampler,\n",
        "                                               num_workers=num_workers\n",
        "                                              )\n",
        "    valid_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                               batch_size=batch_size,\n",
        "                                               sampler=valid_sampler,\n",
        "                                               num_workers=num_workers\n",
        "                                              )\n",
        "\n",
        "    # architecture for correspondence block - 13 objects + backgound = 14 channels for ID masks\n",
        "    correspondence_block = UNET.UNet(n_channels=3, out_channels_id=14, out_channels_uv=256, bilinear=True)\n",
        "    correspondence_block.cuda()\n",
        "    if (transferLearning): # load trained model weights before transfer learning training\n",
        "        correspondence_block.load_state_dict(torch.load(model_dir + 'correspondence_block.pt', map_location=torch.device('cpu')))\n",
        "    \n",
        "    # custom loss function and optimizer\n",
        "    criterion_id = nn.CrossEntropyLoss()\n",
        "    criterion_u = nn.CrossEntropyLoss()\n",
        "    criterion_v = nn.CrossEntropyLoss()\n",
        "\n",
        "    # specify optimizer\n",
        "    optimizer = optim.Adam(correspondence_block.parameters(), lr=3e-4, weight_decay=3e-5)\n",
        "    \n",
        "    #################\n",
        "    ### training loop\n",
        "    n_epochs = epochs # number of epochs to train the model\n",
        "    valid_loss_min = np.Inf  # track change in validation loss\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        \n",
        "        # keep track of training and validation loss\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "        epoch_start_time = time.time()\n",
        "        print(\"------ Epoch \", epoch, \" ---------\")\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        correspondence_block.train()\n",
        "        batch_idx = 0\n",
        "        for _, image, idmask, umask, vmask in train_loader:\n",
        "            # move tensors to GPU if CUDA is available\n",
        "            image, idmask, umask, vmask = image.cuda(), idmask.cuda(), umask.cuda(), vmask.cuda()\n",
        "            # clear the gradients of all optimized variables\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            idmask_pred, umask_pred, vmask_pred = correspondence_block(image)\n",
        "            # calculate the batch loss\n",
        "            loss_id = criterion_id(idmask_pred, idmask)\n",
        "            loss_u = criterion_u(umask_pred, umask)\n",
        "            loss_v = criterion_v(vmask_pred, vmask)\n",
        "            loss = loss_id + loss_u + loss_v\n",
        "            # backward pass: compute gradient of the loss with respect to model parameters\n",
        "            loss.backward()\n",
        "            # perform a single optimization step (parameter update)\n",
        "            optimizer.step()\n",
        "            # update training loss\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # show the progress during each epoch\n",
        "            if(batch_idx % 100 == 0):\n",
        "                print(\"batch progress rate: \", float(batch_idx) / float(len(train_loader)))\n",
        "            batch_idx += 1\n",
        "\n",
        "        ######################\n",
        "        # validate the model #\n",
        "        ######################\n",
        "        correspondence_block.eval()\n",
        "        for _, image, idmask, umask, vmask in valid_loader:\n",
        "            # move tensors to GPU if CUDA is available\n",
        "            image, idmask, umask, vmask = image.cuda(\n",
        "            ), idmask.cuda(), umask.cuda(), vmask.cuda()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            idmask_pred, umask_pred, vmask_pred = correspondence_block(image)\n",
        "            # calculate the batch loss\n",
        "            loss_id = criterion_id(idmask_pred, idmask)\n",
        "            loss_u = criterion_u(umask_pred, umask)\n",
        "            loss_v = criterion_v(vmask_pred, vmask)\n",
        "            loss = loss_id + loss_u + loss_v\n",
        "            # update average validation loss\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "        # calculate average losses\n",
        "        train_loss = train_loss/len(train_loader.sampler)\n",
        "        valid_loss = valid_loss/len(valid_loader.sampler)\n",
        "\n",
        "        # print training/validation statistics\n",
        "        print('Epoch: {}, {:2.2f} sec \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "            epoch, time.time() - epoch_start_time, train_loss, valid_loss))\n",
        "\n",
        "        ## save model if validation loss has decreased\n",
        "        if (valid_loss <= valid_loss_min):\n",
        "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min, valid_loss))\n",
        "            if (transferLearning):\n",
        "                torch.save(correspondence_block.state_dict(), model_dir + 'correspondence_block_transfer.pt')           \n",
        "            else:\n",
        "                torch.save(correspondence_block.state_dict(), model_dir + 'correspondence_block.pt')\n",
        "            \n",
        "            valid_loss_min = valid_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eAeRlFpAmIm"
      },
      "outputs": [],
      "source": [
        "################################\n",
        "#### create_ground_truth.py ####\n",
        "################################\n",
        "\n",
        "import os\n",
        "import re\n",
        "import cv2\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import time\n",
        "\n",
        "# from helper import save_obj, load_obj\n",
        "# import helper\n",
        "\n",
        "## Read the rotation and translation file and return the transform matrix\n",
        "def get_rot_tra(rot_adr, tra_adr):\n",
        "    \"\"\"\n",
        "    Helper function to the read the rotation and translation file\n",
        "        Args:\n",
        "            rot_adr (str): path to the file containing rotation of an object\n",
        "            tra_adr (str): path to the file containing translation of an object\n",
        "        Returns:\n",
        "            rigid transformation (np array): rotation and translation matrix combined: [R, t] dim(3,4)\n",
        "    \"\"\"\n",
        "    rot_matrix = np.loadtxt(rot_adr, skiprows=1)\n",
        "    trans_matrix = np.loadtxt(tra_adr, skiprows=1)\n",
        "    trans_matrix = np.reshape(trans_matrix, (3, 1))\n",
        "    rigid_trnsfrm = np.append(rot_matrix, trans_matrix, axis=1)\n",
        "\n",
        "    return rigid_trnsfrm\n",
        "\n",
        "\n",
        "def fill_holes(idmask, umask, vmask):\n",
        "    \"\"\"\n",
        "    Helper function to fill the holes in id , u and vmasks\n",
        "        Args:\n",
        "                idmask (np.array): id mask whose holes you want to fill\n",
        "        umask (np.array): u mask whose holes you want to fill\n",
        "        vmask (np.array): v mask whose holes you want to fill\n",
        "        Returns:\n",
        "                filled_id_mask (np array): id mask with holes filled\n",
        "        filled_u_mask (np array): u mask with holes filled\n",
        "        filled_id_mask (np array): v mask with holes filled\n",
        "    \"\"\"\n",
        "    idmask = np.array(idmask, dtype='float32')\n",
        "    umask = np.array(umask, dtype='float32')\n",
        "    vmask = np.array(vmask, dtype='float32')\n",
        "    thr, im_th = cv2.threshold(idmask, 0, 255, cv2.THRESH_BINARY_INV)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
        "    res = cv2.morphologyEx(im_th, cv2.MORPH_OPEN, kernel)\n",
        "    im_th = cv2.bitwise_not(im_th)\n",
        "    des = cv2.bitwise_not(res)\n",
        "    mask = np.array(des-im_th, dtype='uint8')\n",
        "    filled_id_mask = cv2.inpaint(idmask, mask, 5, cv2.INPAINT_TELEA)\n",
        "    filled_u_mask = cv2.inpaint(umask, mask, 5, cv2.INPAINT_TELEA)\n",
        "    filled_v_mask = cv2.inpaint(vmask, mask, 5, cv2.INPAINT_TELEA)\n",
        "\n",
        "    return filled_id_mask, filled_u_mask, filled_v_mask\n",
        "\n",
        "\n",
        "## Create the datasets for next steps\n",
        "def create_start_arrays(root_dir, dataset_dir, background_dir, intrnsc_mtrx, class_list, transferLearning=False):\n",
        "    \"\"\"\n",
        "    Helper function to create the Ground Truth ID, U and V masks\n",
        "    Args:\n",
        "        root_dir (str): path to the root directory of the dataset\n",
        "        background_dir (str): path to the background directory\n",
        "        intrnsc_mtrx (array): matrix containing camera intrinsics\n",
        "        classes (dict) : dictionary containing classes and their IDs\n",
        "    Returns:\n",
        "        datasets for the next step\n",
        "    \"\"\"\n",
        "    if (transferLearning):\n",
        "        paths_all_images = load_obj(dataset_dir + \"all_images_adr_transfer\")\n",
        "        train_img_indices = load_obj(dataset_dir + \"train_images_indices_transfer\")\n",
        "    \n",
        "    else:\n",
        "        paths_all_images = load_obj(dataset_dir + \"all_images_adr_test\")\n",
        "        train_img_indices = load_obj(dataset_dir + \"train_images_indices_test\")\n",
        "    \n",
        "    num_train_imgs = len(train_img_indices)\n",
        "    start_time = time.time()\n",
        "    \n",
        "    ## dataset initialization\n",
        "    train_img_set = []\n",
        "    train_trnsfrm_set = []\n",
        "    pt_cld_data_set = []\n",
        "    bgd_img_set = []\n",
        "    \n",
        "    ######################################\n",
        "    ### read data and store them into sets\n",
        "    print(\"reading data\")\n",
        "    for train_img_idx in range(num_train_imgs):\n",
        "\n",
        "        img_adr = paths_all_images[train_img_indices[train_img_idx]]\n",
        "        # print(img_adr)\n",
        "        label = os.path.split(os.path.split(os.path.dirname(img_adr))[0])[1]\n",
        "        # print(label)\n",
        "        regex = re.compile(r'\\d+')\n",
        "        idx = regex.findall(os.path.split(img_adr)[1])[0]\n",
        "        \n",
        "        image = cv2.imread(img_adr) # *******\n",
        "        train_img_set.append(image)\n",
        "\n",
        "        tra_adr = dataset_dir + label + \"/data/tra\" + str(idx) + \".tra\" # *******\n",
        "        rot_adr = dataset_dir + label + \"/data/rot\" + str(idx) + \".rot\" # *******\n",
        "        rigid_trnsfrm = get_rot_tra(rot_adr, tra_adr) # *******\n",
        "        train_trnsfrm_set.append(rigid_trnsfrm)\n",
        "        \n",
        "        # if (train_img_idx % 100 != 0):  # change background for every 99/100 images\n",
        "        bgd_img_adr = background_dir + random.choice(os.listdir(background_dir)) # *******\n",
        "        bgd_img = cv2.imread(bgd_img_adr) # *******\n",
        "        bgd_img_set.append(bgd_img)\n",
        "\n",
        "        # showing progress\n",
        "        if (train_img_idx % 100 == 0):\n",
        "            print(str(train_img_idx) + \"/\" + str(num_train_imgs) + \n",
        "                  \" finished! {:2.2f}\".format(time.time() - start_time), \" sec\")\n",
        "            start_time = time.time()\n",
        "    \n",
        "    for label in class_list:\n",
        "        ptcld_adr = dataset_dir + label + \"/object.xyz\" # *******\n",
        "        try:\n",
        "            pt_cld_data = np.loadtxt(ptcld_adr, skiprows=1) # *******\n",
        "            pt_cld_data_set.append(pt_cld_data)\n",
        "        except:\n",
        "            pt_cld_data_set.append(list())\n",
        "            print(\"XYZ_model of \" + label + \" not found\")\n",
        "    \n",
        "    if (transferLearning):\n",
        "        save_obj(train_img_set, root_dir + \"train_img_set_transfer\")\n",
        "        save_obj(train_trnsfrm_set, root_dir + \"train_trnsfrm_set_transfer\")\n",
        "        save_obj(pt_cld_data_set, root_dir + \"pt_cld_data_set_transfer\")\n",
        "        save_obj(bgd_img_set, root_dir + \"bgd_img_set_transfer\")\n",
        "    \n",
        "    else:\n",
        "        save_obj(train_img_set, root_dir + \"train_img_set\")\n",
        "        save_obj(train_trnsfrm_set, root_dir + \"train_trnsfrm_set\")\n",
        "        save_obj(pt_cld_data_set, root_dir + \"pt_cld_data_set\")\n",
        "        save_obj(bgd_img_set, root_dir + \"bgd_img_set\")\n",
        "\n",
        "    return train_img_set, train_trnsfrm_set, pt_cld_data_set, bgd_img_set\n",
        "\n",
        "\n",
        "## Create the ground-truth ID, U and V masks for training images\n",
        "def create_GT_masks_new(root_dir, dataset_dir, background_dir, intrnsc_mtrx, classes,\n",
        "                    train_img_set, train_trnsfrm_set, pt_cld_data_set, bgd_img_set,\n",
        "                    subset_idx, subset_size, transferLearning=False):\n",
        "    \"\"\"\n",
        "    sub-datasets:\n",
        "        train_img_set\n",
        "        bgd_img_set\n",
        "        ID_mask_set\n",
        "        U_mask_set\n",
        "        V_mask_set\n",
        "        changed_bgd_set\n",
        "\n",
        "    Helper function to create the Ground Truth ID, U and V masks\n",
        "    Args:\n",
        "        root_dir (str): path to the root directory of the dataset\n",
        "        background_dir (str): path to the background directory\n",
        "        intrnsc_mtrx (array): matrix containing camera intrinsics\n",
        "        classes (dict) : dictionary containing classes and their IDs\n",
        "        datasets ......\n",
        "    Outputs:\n",
        "        Saves the masks to their respective directories\n",
        "    \"\"\"\n",
        "    if (transferLearning):\n",
        "        paths_all_images = load_obj(dataset_dir + \"all_images_adr_transfer\")\n",
        "        train_img_indices = load_obj(dataset_dir + \"train_images_indices_transfer\")\n",
        "    \n",
        "    else:\n",
        "        paths_all_images = load_obj(dataset_dir + \"all_images_adr_test\")\n",
        "        train_img_indices = load_obj(dataset_dir + \"train_images_indices_test\")\n",
        "    \n",
        "    num_train_imgs = len(train_img_indices)\n",
        "    start_time = time.time()\n",
        "    \n",
        "    ID_mask_set = []\n",
        "    U_mask_set = []\n",
        "    V_mask_set = []\n",
        "    changed_bgd_set = []\n",
        "    \n",
        "    ###############\n",
        "    ### handle data  \n",
        "    print(\"handling data\")\n",
        "    for train_img_idx in range(subset_idx * subset_size, (subset_idx + 1) * subset_size):\n",
        "        \n",
        "        print(train_img_idx - subset_idx * subset_size)\n",
        "        img_adr = paths_all_images[train_img_indices[train_img_idx]]\n",
        "        label = os.path.split(os.path.split(os.path.dirname(img_adr))[0])[1]\n",
        "        regex = re.compile(r'\\d+')\n",
        "        idx = regex.findall(os.path.split(img_adr)[1])[0]\n",
        "\n",
        "        # image = cv2.imread(img_adr) # *******\n",
        "        image = train_img_set[train_img_idx - subset_idx * subset_size]\n",
        "        image_h = image.shape[0]\n",
        "        image_w = image.shape[1]\n",
        "        ID_mask = np.zeros((image_h, image_w))\n",
        "        U_mask = np.zeros((image_h, image_w))\n",
        "        V_mask = np.zeros((image_h, image_w))\n",
        "\n",
        "        suffix = \"_transfer.png\" if transferLearning else \"_test.png\"\n",
        "        ID_mask_adr = dataset_dir + label + \"/ground_truth/IDmasks/color\" + str(idx) + suffix ########\n",
        "        U_mask_adr = dataset_dir + label +  \"/ground_truth/Umasks/color\" + str(idx) + suffix ########\n",
        "        V_mask_adr = dataset_dir + label +  \"/ground_truth/Vmasks/color\" + str(idx) + suffix ########\n",
        "\n",
        "        # tra_adr = root_dir + label + \"/data/tra\" + str(idx) + \".tra\" # *******\n",
        "        # rot_adr = root_dir + label + \"/data/rot\" + str(idx) + \".rot\" # *******\n",
        "        # rigid_trnsfrm = get_rot_tra(rot_adr, tra_adr) # *******\n",
        "        rigid_trnsfrm = train_trnsfrm_set[train_img_idx]\n",
        "\n",
        "        # Read point cloud data\n",
        "        # ptcld_adr = root_dir + label + \"/object.xyz\" # *******\n",
        "        # pt_cld_data = np.loadtxt(ptcld_adr, skiprows=1, usecols=(0, 1, 2)) # *******\n",
        "        pt_cld_data = pt_cld_data_set[classes[label] - 1]\n",
        "        pt_cld_data = pt_cld_data[ :, 0 : 3]\n",
        "        ones = np.ones((pt_cld_data.shape[0], 1))\n",
        "        homo_coord = np.append(pt_cld_data[:, :3], ones, axis=1)\n",
        "\n",
        "        # Perspective Projection to obtain 2D coordinates for masks\n",
        "        homo_2D = intrnsc_mtrx @ (rigid_trnsfrm @ homo_coord.T)\n",
        "        coord_2D = homo_2D[:2, :] / homo_2D[2, :]\n",
        "        coord_2D = ((np.floor(coord_2D)).T).astype(int)\n",
        "        x_2d = np.clip(coord_2D[:, 0], 0, 639)\n",
        "        y_2d = np.clip(coord_2D[:, 1], 0, 479)\n",
        "        ID_mask[y_2d, x_2d] = classes[label]\n",
        "\n",
        "        if (train_img_idx % 100 != 0):  # change background for every 99/100 images\n",
        "            # bgd_img_adr = background_dir + random.choice(os.listdir(background_dir)) # *******\n",
        "            # bgd_img = cv2.imread(bgd_img_adr) # *******\n",
        "            changed_bgd_img = bgd_img_set[train_img_idx - subset_idx * subset_size]\n",
        "            changed_bgd_img = cv2.resize(changed_bgd_img, (image_w, image_h), interpolation=cv2.INTER_AREA)\n",
        "            changed_bgd_img[y_2d, x_2d, :] = image[y_2d, x_2d, :]\n",
        "            changed_bgd_set.append(changed_bgd_img)\n",
        "        \n",
        "        else:\n",
        "            changed_bgd_img = image\n",
        "            changed_bgd_set.append(changed_bgd_img)\n",
        "\n",
        "        changed_bgd_adr = dataset_dir + label + \"/changed_background/color\" + str(idx) + suffix ########\n",
        "        mpimg.imsave(changed_bgd_adr, changed_bgd_img) ########\n",
        "\n",
        "        # Generate Ground Truth UV Maps\n",
        "        centre = np.mean(pt_cld_data, axis=0)\n",
        "        length = np.sqrt( (centre[0] - pt_cld_data[:, 0])**2 +\n",
        "                          (centre[1] - pt_cld_data[:, 1])**2 +\n",
        "                          (centre[2] - pt_cld_data[:, 2])**2 )\n",
        "        unit_vector = [ (pt_cld_data[:, 0] - centre[0]) / length,\n",
        "                        (pt_cld_data[:, 1] - centre[1]) / length,\n",
        "                        (pt_cld_data[:, 2] - centre[2]) / length ]\n",
        "        U = 0.5 + (np.arctan2(unit_vector[2], unit_vector[0]) / (2 * np.pi))\n",
        "        V = 0.5 - (np.arcsin(unit_vector[1]) / np.pi)\n",
        "        U_mask[y_2d, x_2d] = U\n",
        "        V_mask[y_2d, x_2d] = V\n",
        "\n",
        "        # Saving ID, U and V masks after using the fill holes function\n",
        "        ID_mask, U_mask, V_mask = fill_holes(ID_mask, U_mask, V_mask)\n",
        "        ID_mask_set.append(ID_mask)\n",
        "        U_mask_set.append(U_mask)\n",
        "        V_mask_set.append(V_mask)\n",
        "        cv2.imwrite(ID_mask_adr, ID_mask) ########\n",
        "        mpimg.imsave(U_mask_adr, U_mask, cmap='gray') ########\n",
        "        mpimg.imsave(V_mask_adr, V_mask, cmap='gray') ########\n",
        "\n",
        "        # showing progress\n",
        "        if (train_img_idx % 100 == 0):\n",
        "            print(str(train_img_idx) + \"/\" + str(num_train_imgs) + \n",
        "                  \" finished! {:2.2f}\".format(time.time() - start_time), \" sec\")\n",
        "            start_time = time.time()\n",
        "    \n",
        "    \n",
        "    print(len(ID_mask_set))\n",
        "    print(len(U_mask_set))\n",
        "    print(len(V_mask_set))\n",
        "    print(len(changed_bgd_set))\n",
        "    \n",
        "    ## save data\n",
        "    print(\"saving data\")\n",
        "    for train_img_idx in range(subset_idx * subset_size, (subset_idx + 1) * subset_size):\n",
        "        \n",
        "        # print(train_img_idx - subset_idx * subset_size)\n",
        "        img_adr = paths_all_images[train_img_indices[train_img_idx]]\n",
        "        label = os.path.split(os.path.split(os.path.dirname(img_adr))[0])[1]\n",
        "        regex = re.compile(r'\\d+')\n",
        "        idx = regex.findall(os.path.split(img_adr)[1])[0]\n",
        "\n",
        "        suffix = \"_transfer.png\" if transferLearning else \"_test.png\"\n",
        "        ID_mask_adr = dataset_dir + label + \"/ground_truth/IDmasks/color\" + str(idx) + suffix ########\n",
        "        U_mask_adr = dataset_dir + label +  \"/ground_truth/Umasks/color\" + str(idx) + suffix ########\n",
        "        V_mask_adr = dataset_dir + label +  \"/ground_truth/Vmasks/color\" + str(idx) + suffix ########\n",
        "\n",
        "        if (train_img_idx % 100 != 0):\n",
        "            changed_bgd_adr = dataset_dir + label + \"/changed_background/color\" + str(idx) + suffix ########\n",
        "            mpimg.imsave(changed_bgd_adr, changed_bgd_set[train_img_idx - subset_idx * subset_size]) ########\n",
        "\n",
        "        cv2.imwrite(ID_mask_adr, ID_mask_set[train_img_idx - subset_idx * subset_size]) ########\n",
        "        mpimg.imsave(U_mask_adr, U_mask_set[train_img_idx - subset_idx * subset_size], cmap='gray') ########\n",
        "        mpimg.imsave(V_mask_adr, V_mask_set[train_img_idx - subset_idx * subset_size], cmap='gray') ########\n",
        "\n",
        "        # showing progress\n",
        "        if (train_img_idx % 100 == 0):\n",
        "            print(str(train_img_idx) + \"/\" + str(num_train_imgs) + \n",
        "                  \" finished! {:2.2f}\".format(time.time() - start_time), \" sec\")\n",
        "            start_time = time.time()\n",
        "    middle = \"_transfer_\" if transferLearning else \"_\"\n",
        "    save_obj(ID_mask_set, root_dir + \"ID_mask_set\" + middle + str(subset_idx))\n",
        "    save_obj(U_mask_set, root_dir + \"U_mask_set\" + middle + str(subset_idx))\n",
        "    save_obj(V_mask_set, root_dir + \"V_mask_set\" + middle + str(subset_idx))\n",
        "    save_obj(changed_bgd_set, root_dir + \"changed_bgd_set\" + middle + str(subset_idx))\n",
        "\n",
        "    return ID_mask_set, U_mask_set, V_mask_set, changed_bgd_set\n",
        "    \n",
        "\n",
        "\n",
        "## Create the datasets generated from last step\n",
        "def create_end_arrays(root_dir, dataset_dir, subset_idx, subset_size, transferLearning=False):\n",
        "    if (transferLearning):\n",
        "        paths_all_images = load_obj(dataset_dir + \"all_images_adr_transfer\")\n",
        "        train_img_indices = load_obj(dataset_dir + \"train_images_indices_transfer\")\n",
        "    \n",
        "    else:\n",
        "        paths_all_images = load_obj(dataset_dir + \"all_images_adr_test\")\n",
        "        train_img_indices = load_obj(dataset_dir + \"train_images_indices_test\")\n",
        "    \n",
        "    num_train_imgs = len(train_img_indices)\n",
        "    start_time = time.time()\n",
        "\n",
        "    changed_bgd_set = []\n",
        "    ID_mask_set = []\n",
        "    U_mask_set = []\n",
        "    V_mask_set = []\n",
        "\n",
        "    ## save data\n",
        "    print(\"saving data\")\n",
        "    # for train_img_idx in range(num_train_imgs):\n",
        "    for train_img_idx in range(subset_idx * subset_size, (subset_idx + 1) * subset_size):\n",
        "\n",
        "        # print(train_img_idx - subset_idx * subset_size)\n",
        "        img_adr = paths_all_images[train_img_indices[train_img_idx]]\n",
        "        label = os.path.split(os.path.split(os.path.dirname(img_adr))[0])[1]\n",
        "        regex = re.compile(r'\\d+')\n",
        "        idx = regex.findall(os.path.split(img_adr)[1])[0]\n",
        "\n",
        "        suffix = \"_transfer.png\" if transferLearning else \"_test.png\"\n",
        "        ID_mask_adr = dataset_dir + label + \"/ground_truth/IDmasks/color\" + str(idx) + suffix ########\n",
        "        U_mask_adr = dataset_dir + label +  \"/ground_truth/Umasks/color\" + str(idx) + suffix ########\n",
        "        V_mask_adr = dataset_dir + label +  \"/ground_truth/Vmasks/color\" + str(idx) + suffix ########\n",
        "        changed_bgd_adr = dataset_dir + label + \"/changed_background/color\" + str(idx) + suffix ########\n",
        "        \n",
        "        image = cv2.imread(changed_bgd_adr)\n",
        "        image = cv2.resize(image, (image.shape[1]//2, image.shape[0]//2), interpolation=cv2.INTER_AREA)\n",
        "        changed_bgd_set.append(image) ########\n",
        "        \n",
        "        ID_mask = cv2.imread(ID_mask_adr, cv2.IMREAD_GRAYSCALE)\n",
        "        ID_mask = cv2.resize(ID_mask, (ID_mask.shape[1]//2, ID_mask.shape[0]//2), interpolation=cv2.INTER_AREA)\n",
        "        ID_mask_set.append(ID_mask) ########\n",
        "        \n",
        "        U_mask = cv2.imread(U_mask_adr, cv2.IMREAD_GRAYSCALE)\n",
        "        U_mask = cv2.resize(U_mask, (U_mask.shape[1]//2, U_mask.shape[0]//2), interpolation=cv2.INTER_AREA)\n",
        "        U_mask_set.append(U_mask) ########\n",
        "        \n",
        "        V_mask = cv2.imread(V_mask_adr, cv2.IMREAD_GRAYSCALE)\n",
        "        V_mask = cv2.resize(V_mask, (V_mask.shape[1]//2, V_mask.shape[0]//2), interpolation=cv2.INTER_AREA)\n",
        "        V_mask_set.append(V_mask) ########\n",
        "\n",
        "        # showing progress\n",
        "        if (train_img_idx % 50 == 0):\n",
        "            print(image.shape)\n",
        "            print(ID_mask.shape)\n",
        "            print(U_mask.shape)\n",
        "            print(V_mask.shape)\n",
        "            print(str(train_img_idx) + \"/\" + str(num_train_imgs) + \n",
        "                  \" finished! {:2.2f}\".format(time.time() - start_time), \" sec\")\n",
        "            start_time = time.time()\n",
        "    \n",
        "    print(\"end of saving data\")\n",
        "    middle = \"_transfer_\" if transferLearning else \"_\"\n",
        "    save_obj(ID_mask_set, root_dir + \"ID_mask_set_small\" + middle + str(subset_idx))\n",
        "    save_obj(U_mask_set, root_dir + \"U_mask_set_small\" + middle  + str(subset_idx))\n",
        "    save_obj(V_mask_set, root_dir + \"V_mask_set_small\" + middle  + str(subset_idx))\n",
        "    save_obj(changed_bgd_set, root_dir + \"changed_bgd_set_small\" + middle  + str(subset_idx))\n",
        "\n",
        "    return ID_mask_set, U_mask_set, V_mask_set, changed_bgd_set\n",
        "\n",
        "\n",
        "def create_end_arrays_2(root_dir, dataset_dir, subset_idx, subset_size, transferLearning):\n",
        "    if (transferLearning):\n",
        "        paths_all_images = load_obj(dataset_dir + \"all_images_adr_transfer\")\n",
        "        train_img_indices = load_obj(dataset_dir + \"train_images_indices_transfer\")\n",
        "    else:\n",
        "        paths_all_images = load_obj(dataset_dir + \"all_images_adr_test\")\n",
        "        train_img_indices = load_obj(dataset_dir + \"train_images_indices_test\")\n",
        "    \n",
        "    num_train_imgs = len(train_img_indices)\n",
        "    start_time = time.time()\n",
        "\n",
        "    real_set = []\n",
        "    rendered_set = []\n",
        "\n",
        "    ## save data\n",
        "    print(\"saving data\")\n",
        "    # for train_img_idx in range(num_train_imgs):\n",
        "    for train_img_idx in range(subset_idx * subset_size, (subset_idx + 1) * subset_size):\n",
        "\n",
        "        # print(train_img_idx - subset_idx * subset_size)\n",
        "        img_adr = paths_all_images[train_img_indices[train_img_idx]]\n",
        "        label = os.path.split(os.path.split(os.path.dirname(img_adr))[0])[1]\n",
        "        regex = re.compile(r'\\d+')\n",
        "        idx = regex.findall(os.path.split(img_adr)[1])[0]\n",
        "\n",
        "        suffix = \"_transfer.png\" if transferLearning else \"_test.png\"\n",
        "        real = cv2.imread(dataset_dir + label + '/pose_refinement/real/color' + str(idx) + suffix)\n",
        "        real_set.append(real) ########\n",
        "        \n",
        "        rendered = cv2.imread(dataset_dir + label + '/pose_refinement/rendered/color' + str(idx) + suffix, cv2.IMREAD_GRAYSCALE) #######\n",
        "        rendered_set.append(rendered) ########\n",
        "    \n",
        "\n",
        "        # showing progress\n",
        "        if (train_img_idx % 50 == 0):\n",
        "            print(real.shape)\n",
        "            print(rendered.shape)\n",
        "            print(str(train_img_idx) + \"/\" + str(num_train_imgs) + \n",
        "                  \" finished! {:2.2f}\".format(time.time() - start_time), \" sec\")\n",
        "            start_time = time.time()\n",
        "    \n",
        "    print(\"end of saving data\")\n",
        "    if (transferLearning):\n",
        "        save_obj(real_set, root_dir + \"real_set_transfer_\" + str(subset_idx))\n",
        "        save_obj(rendered_set, root_dir + \"rendered_set_transfer_\" + str(subset_idx))\n",
        "    else:\n",
        "        save_obj(real_set, root_dir + \"real_set_\" + str(subset_idx))\n",
        "        save_obj(rendered_set, root_dir + \"rendered_set_\" + str(subset_idx))\n",
        "\n",
        "    return real_set, rendered_set\n",
        "\n",
        "\n",
        "## Create and store the ground-truth ID, U and V masks for training images (original code)\n",
        "def create_GT_masks(dataset_dir, background_dir, intrnsc_mtrx, classes):\n",
        "    \"\"\"\n",
        "    Helper function to create the Ground Truth ID,U and V masks\n",
        "        Args:\n",
        "        root_dir (str): path to the root directory of the dataset\n",
        "        background_dir(str): path t\n",
        "        intrnsc_mtrx (array): matrix containing camera intrinsics\n",
        "        classes (dict) : dictionary containing classes and their ids\n",
        "        Saves the masks to their respective directories\n",
        "    \"\"\"\n",
        "    list_all_images = load_obj(dataset_dir + \"all_images_adr_test\")\n",
        "    training_images_idx = load_obj(dataset_dir + \"train_images_indices_test\")\n",
        "    num_train_imgs = len(training_images_idx)\n",
        "    # for i in range(5):\n",
        "    #     print(list_all_images[training_images_idx[i]])\n",
        "\n",
        "    start_time = time.time()\n",
        "    for i in range(num_train_imgs):\n",
        "        img_adr = list_all_images[training_images_idx[i]]\n",
        "        label = os.path.split(os.path.split(os.path.dirname(img_adr))[0])[1]\n",
        "        regex = re.compile(r'\\d+')\n",
        "        idx = regex.findall(os.path.split(img_adr)[1])[0]\n",
        "\n",
        "        # showing progress\n",
        "        if (i % 100 == 0):\n",
        "            print(str(i) + \"/\" + str(num_train_imgs) + \n",
        "                  \" finished! {:2.2f}\".format(time.time() - start_time), \" sec\")\n",
        "            start_time = time.time()\n",
        "\n",
        "        image = cv2.imread(img_adr)\n",
        "        ID_mask = np.zeros((image.shape[0], image.shape[1]))\n",
        "        U_mask = np.zeros((image.shape[0], image.shape[1]))\n",
        "        V_mask = np.zeros((image.shape[0], image.shape[1]))\n",
        "\n",
        "        ID_mask_file = dataset_dir + label + \"/ground_truth/IDmasks/color\" + str(idx) + \"_test.png\"\n",
        "        U_mask_file = dataset_dir + label + \"/ground_truth/Umasks/color\" + str(idx) + \"_test.png\"\n",
        "        V_mask_file = dataset_dir + label + \"/ground_truth/Vmasks/color\" + str(idx) + \"_test.png\"\n",
        "\n",
        "        tra_adr = dataset_dir + label + \"/data/tra\" + str(idx) + \".tra\"\n",
        "        rot_adr = dataset_dir + label + \"/data/rot\" + str(idx) + \".rot\"\n",
        "        rigid_transformation = get_rot_tra(rot_adr, tra_adr)\n",
        "\n",
        "        # Read point Point Cloud Data\n",
        "        ptcld_file = dataset_dir + label + \"/object.xyz\"\n",
        "        pt_cld_data = np.loadtxt(ptcld_file, skiprows=1, usecols=(0, 1, 2))\n",
        "        ones = np.ones((pt_cld_data.shape[0], 1))\n",
        "        homogenous_coordinate = np.append(pt_cld_data[:, :3], ones, axis=1)\n",
        "\n",
        "        # Perspective Projection to obtain 2D coordinates for masks\n",
        "        homogenous_2D = intrnsc_mtrx @ (\n",
        "            rigid_transformation @ homogenous_coordinate.T)\n",
        "        coord_2D = homogenous_2D[:2, :] / homogenous_2D[2, :]\n",
        "        coord_2D = ((np.floor(coord_2D)).T).astype(int)\n",
        "        x_2d = np.clip(coord_2D[:, 0], 0, 639)\n",
        "        y_2d = np.clip(coord_2D[:, 1], 0, 479)\n",
        "        ID_mask[y_2d, x_2d] = classes[label]\n",
        "\n",
        "        \n",
        "        background_img_adr = background_dir + random.choice(os.listdir(background_dir))\n",
        "        background_img = cv2.imread(background_img_adr)\n",
        "        background_img = cv2.resize(background_img, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_AREA)\n",
        "        background_img[y_2d, x_2d, :] = image[y_2d, x_2d, :]\n",
        "        background_adr = dataset_dir + label + \"/changed_background/color\" + str(idx) + \"_test.png\"\n",
        "        if (i % 100 != 0):  # change background for every 99/100 images\n",
        "            mpimg.imsave(background_adr, background_img)\n",
        "        \n",
        "        else:\n",
        "            print(image.shape)\n",
        "            print(background_img.shape)\n",
        "            mpimg.imsave(background_adr, image)\n",
        "\n",
        "        # Generate Ground Truth UV Maps\n",
        "        centre = np.mean(pt_cld_data, axis=0)\n",
        "        length = np.sqrt((centre[0] - pt_cld_data[:, 0])**2 +\n",
        "                         (centre[1] - pt_cld_data[:, 1])**2 +\n",
        "                         (centre[2] - pt_cld_data[:, 2])**2)\n",
        "        unit_vector = [(pt_cld_data[:, 0] - centre[0]) / length,\n",
        "                       (pt_cld_data[:, 1] - centre[1]) / length,\n",
        "                       (pt_cld_data[:, 2] - centre[2]) / length]\n",
        "        U = 0.5 + (np.arctan2(unit_vector[2], unit_vector[0])/(2*np.pi))\n",
        "        V = 0.5 - (np.arcsin(unit_vector[1])/np.pi)\n",
        "        U_mask[y_2d, x_2d] = U\n",
        "        V_mask[y_2d, x_2d] = V\n",
        "\n",
        "        # Saving ID, U and V masks after using the fill holes function\n",
        "        ID_mask, U_mask, V_mask = fill_holes(ID_mask, U_mask, V_mask)\n",
        "        cv2.imwrite(ID_mask_file, ID_mask)\n",
        "        mpimg.imsave(U_mask_file, U_mask, cmap='gray')\n",
        "        mpimg.imsave(V_mask_file, V_mask, cmap='gray')\n",
        "\n",
        "\n",
        "def create_UV_XYZ_dictionary(dataset_dir, class_list):\n",
        "\n",
        "    # create a dictionary for UV to XYZ correspondence\n",
        "    for label in class_list:\n",
        "        try: \n",
        "            ptcld_file = dataset_dir + label + \"/object.xyz\"\n",
        "            pt_cld_data = np.loadtxt(ptcld_file, skiprows=1, usecols=(0, 1, 2))\n",
        "            \n",
        "            # calculate u and v coordinates from the xyz point cloud file\n",
        "            centre = np.mean(pt_cld_data, axis=0)\n",
        "            length = np.sqrt((centre[0] - pt_cld_data[:, 0])**2 +\n",
        "                            (centre[1] - pt_cld_data[:, 1])**2 +\n",
        "                            (centre[2] - pt_cld_data[:, 2])**2)\n",
        "            unit_vector = [(pt_cld_data[:, 0] - centre[0]) / length,\n",
        "                        (pt_cld_data[:, 1] - centre[1]) / length,\n",
        "                        (pt_cld_data[:, 2] - centre[2]) / length]\n",
        "            u_coord = 0.5 + (np.arctan2(unit_vector[2], unit_vector[0]) / (2 * np.pi))\n",
        "            v_coord = 0.5 - (np.arcsin(unit_vector[1]) / np.pi)\n",
        "            u_coord = (u_coord * 255).astype(int)\n",
        "            v_coord = (v_coord * 255).astype(int)\n",
        "            \n",
        "            # save the mapping as a pickle file\n",
        "            dct = {}\n",
        "            for u, v, xyz in zip(u_coord, v_coord, pt_cld_data):\n",
        "                key = (u, v)\n",
        "                if key not in dct:\n",
        "                    dct[key] = xyz\n",
        "            save_obj(dct, dataset_dir + label + \"/UV-XYZ_mapping\")\n",
        "            \n",
        "            print(\"UV-XYZ_mapping of \" + label + \" saved\")\n",
        "        \n",
        "        except:\n",
        "            print(\"XYZ_model of \" + label + \" not found\")\n",
        "\n",
        "\n",
        "# Create directories to store generated data\n",
        "def create_dataset_dirs(root_dir, classes):\n",
        "    for label in classes:  # create directories to store data\n",
        "        os.makedirs(root_dir + label + \"/predicted_pose\", exist_ok=True)\n",
        "        os.makedirs(root_dir + label + \"/ground_truth\", exist_ok=True)\n",
        "        os.makedirs(root_dir + label + \"/ground_truth/IDmasks\", exist_ok=True)\n",
        "        os.makedirs(root_dir + label + \"/ground_truth/Umasks\", exist_ok=True)\n",
        "        os.makedirs(root_dir + label + \"/ground_truth/Vmasks\", exist_ok=True)\n",
        "        os.makedirs(root_dir + label + \"/changed_background\", exist_ok=True)\n",
        "        os.makedirs(root_dir + label + \"/pose_refinement\", exist_ok=True)\n",
        "        os.makedirs(root_dir + label + \"/pose_refinement/real\", exist_ok=True)\n",
        "        os.makedirs(root_dir + label + \"/pose_refinement/rendered\", exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kLeP0asElQ7"
      },
      "outputs": [],
      "source": [
        "##################\n",
        "#### train.py ####\n",
        "##################\n",
        "\n",
        "import os\n",
        "import re\n",
        "import cv2\n",
        "import pickle\n",
        "import argparse\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "random.seed(1234)\n",
        "np.random.seed(69)\n",
        "\n",
        "# import helper\n",
        "# import pose_block\n",
        "# import create_renderings\n",
        "# import pose_refinement\n",
        "# import correspondence_block\n",
        "# import create_ground_truth\n",
        "\n",
        "# from helper import save_obj\n",
        "# from pose_block import initial_pose_estimation\n",
        "# from create_renderings import create_refinement_inputs\n",
        "# from pose_refinement import train_pose_refinement\n",
        "# from correspondence_block import train_correspondence_block\n",
        "# from create_ground_truth import create_GT_masks, create_UV_XYZ_dictionary, create_dataset_dirs\n",
        "\n",
        "### global parameter setting\n",
        "Path_root_dir = \"/content/gdrive/MyDrive/path/of/folder/where/you/put/your/dataset\"\n",
        "Path_bgd_data_dir = \"./val2017/\"\n",
        "Path_model_dir = \"/content/gdrive/MyDrive/path/of/folder/where/you/put/your/saved/model/parameters\"\n",
        "Path_dataset_dir = \"./Dataset/\"\n",
        "# Path_dataset_dir = \"./Dataset/TransferLearning/\" # for transfer learning\n",
        "Split_value = 0.075 # ratio of training data\n",
        "\n",
        "# Intrinsic Parameters of the Camera\n",
        "fx = 572.41140\n",
        "px = 325.26110\n",
        "fy = 573.57043\n",
        "py = 242.04899\n",
        "\n",
        "## argument parsing\n",
        "# parser = argparse.ArgumentParser(description='Script to create the Ground Truth masks')\n",
        "# parser.add_argument(\"--root_dir\", default=path_data_set, help=\"path to dataset directory\")\n",
        "# parser.add_argument(\"--bgd_dir\", default=path_bgd_data, help=\"path to background images dataset directory\")\n",
        "# parser.add_argument(\"--split\", default=Split_value, help=\"train:test split ratio\")\n",
        "# args = parser.parse_args()\n",
        "args = {'root_dir': Path_root_dir,\n",
        "        'bgd_dir': Path_bgd_data_dir,\n",
        "        'dataset_dir': Path_dataset_dir,\n",
        "        'model_dir': Path_model_dir,\n",
        "        'split': Split_value}\n",
        "\n",
        "root_dir = args['root_dir']\n",
        "background_dir = args['bgd_dir']\n",
        "dataset_dir = args['dataset_dir']\n",
        "model_dir = args['model_dir']\n",
        "\n",
        "## 1st time to collect indices\n",
        "# paths_all_images = []\n",
        "# for root, dirs, files in os.walk(dataset_dir):\n",
        "#     for file_name in files:\n",
        "#         if file_name.endswith(\".png\") and file_name.startswith(\"color\"):  # image that exists\n",
        "#             # avoid repeated file path appended to the list due to Gdrive issue\n",
        "#             if (os.path.join(root, file_name) in paths_all_images):\n",
        "#                 continue\n",
        "#             else:\n",
        "#                 if(\"/data/\" in os.path.join(root, file_name)):\n",
        "#                     paths_all_images.append(os.path.join(root, file_name))\n",
        "\n",
        "# save_obj(paths_all_images, root_dir + \"all_images_adr_transfer\")\n",
        "# save_obj(paths_all_images, dataset_dir + \"all_images_adr_transfer\")\n",
        "\n",
        "## after 1st time to collect indices\n",
        "# paths_all_images = load_obj(root_dir + \"all_images_adr_transfer\")\n",
        "paths_all_images = load_obj(dataset_dir + \"all_images_adr_test\")\n",
        "\n",
        "\n",
        "num_images = len(paths_all_images)\n",
        "indices = list(range(num_images))\n",
        "\n",
        "## 1st time to collect indices\n",
        "# np.random.shuffle(indices)\n",
        "# num_split = int(np.floor(args['split'] * num_images))\n",
        "# indices_train, indices_test = indices[:num_split], indices[num_split:]\n",
        "# save_obj(indices_train, root_dir + \"train_images_indices_transfer\")\n",
        "# save_obj(indices_test, root_dir + \"test_images_indices_transfer\")\n",
        "# save_obj(indices_train, dataset_dir + \"train_images_indices_transfer\")\n",
        "# save_obj(indices_test, dataset_dir + \"test_images_indices_transfer\")\n",
        "\n",
        "## after 1st time to collect indices\n",
        "indices_train = load_obj(dataset_dir + \"train_images_indices_test\")\n",
        "indices_test = load_obj(dataset_dir + \"test_images_indices_test\")\n",
        "# indices_train = load_obj(root_dir + \"train_images_indices_transfer\")\n",
        "# indices_test = load_obj(root_dir + \"test_images_indices_transfer\")\n",
        "\n",
        "print(\"Total number of images: \", num_images)\n",
        "print(\"Total number of training images: \", len(indices_train))\n",
        "# print(\"Total number of testing images: \", len(indices_test))\n",
        "\n",
        "class_list = ['ape', 'benchviseblue', 'cam', 'can', 'cat', 'driller', 'duck', 'eggbox', 'glue',\n",
        "              'holepuncher', 'iron', 'lamp', 'phone']\n",
        "\n",
        "create_dataset_dirs(dataset_dir, class_list)\n",
        "\n",
        "intrnsc_mtrx = np.array([[fx, 0, px],\n",
        "                         [0, fy, py],\n",
        "                         [0,  0,  1]])\n",
        "\n",
        "classes = {'ape': 1, 'benchviseblue': 2, 'cam': 3, 'can': 4, 'cat': 5, 'driller': 6,\n",
        "           'duck': 7, 'eggbox': 8, 'glue': 9, 'holepuncher': 10, 'iron': 11, 'lamp': 12, 'phone': 13}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(paths_all_images)"
      ],
      "metadata": {
        "id": "ZXq4d5tD53eN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "oChGBbhKGXlB"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# print(\"------ Start creating ground truth ------\")\n",
        "# train_img_set, train_trnsfrm_set, pt_cld_data_set, bgd_img_set = create_start_arrays(root_dir, dataset_dir, background_dir, intrnsc_mtrx, class_list, transferLearning=True)\n",
        "\n",
        "# save_obj(train_img_set, dataset_dir + \"train_img_set_transfer\")\n",
        "# save_obj(train_trnsfrm_set, dataset_dir + \"train_trnsfrm_set_transfer\")\n",
        "# save_obj(pt_cld_data_set, dataset_dir + \"pt_cld_data_set_transfer\")\n",
        "# save_obj(bgd_img_set, dataset_dir + \"bgd_img_set_transfer\")\n",
        "\n",
        "# np.save( root_dir + \"train_img_set_transfer\", train_img_set)\n",
        "# np.save( root_dir + \"train_trnsfrm_set_transfer\", train_trnsfrm_set)\n",
        "# np.save( root_dir + \"pt_cld_data_set_transfer\", np.array(pt_cld_data_set, dtype='object'))\n",
        "# np.save( root_dir + \"bgd_img_set_transfer\", bgd_img_set)\n",
        "\n",
        "## correct some error start ##\n",
        "# pt_cld_data_set = []\n",
        "# for label in class_list:\n",
        "#     ptcld_adr = dataset_dir + label + \"/object.xyz\" # *******\n",
        "#     pt_cld_data = np.loadtxt(ptcld_adr, skiprows=1) # *******\n",
        "#     pt_cld_data_set.append(pt_cld_data)\n",
        "\n",
        "# save_obj(pt_cld_data_set, root_dir + \"pt_cld_data_set\")\n",
        "# np.save( root_dir + \"pt_cld_data_set\", np.array(pt_cld_data_set, dtype='object'))\n",
        "## correct some error end ##\n",
        "\n",
        "'''\n",
        "train_img_set = load_obj(dataset_dir + \"train_img_set_transfer\")\n",
        "train_trnsfrm_set = load_obj(dataset_dir + \"train_trnsfrm_set_transfer\")\n",
        "train_pt_cld_set = load_obj(dataset_dir + \"pt_cld_data_set_transfer\")\n",
        "bgd_img_set = load_obj(root_dir + \"bgd_img_set_transfer\")\n",
        "\n",
        "ID_mask_setset = []\n",
        "U_mask_setset = []\n",
        "V_mask_setset = []\n",
        "changed_bgd_setset = []\n",
        "num_subset = 1\n",
        "subset_size = int(len(train_img_set) / num_subset)\n",
        "for subset_idx in range(num_subset):\n",
        "    print(\"subset_idx: \", subset_idx)\n",
        "    train_img_set = load_obj(root_dir + \"train_img_set_transfer\")\n",
        "    train_img_set = train_img_set[subset_idx * subset_size : (subset_idx + 1) * subset_size]\n",
        "    bgd_img_set = load_obj(root_dir + \"bgd_img_set_transfer\")\n",
        "    bgd_img_set = bgd_img_set[subset_idx * subset_size : (subset_idx + 1) * subset_size]\n",
        "    print(len(train_img_set))\n",
        "    print(len(bgd_img_set))\n",
        "    # ID_mask_set, U_mask_set, V_mask_set, changed_bgd_set = create_GT_masks(root_dir, dataset_dir, background_dir, intrnsc_mtrx, classes,\n",
        "                                                                        #    train_img_set, train_trnsfrm_set, train_pt_cld_set, bgd_img_set,\n",
        "                                                                        #    subset_idx, subset_size)\n",
        "    create_GT_masks_new(root_dir, dataset_dir, background_dir, intrnsc_mtrx, classes,\n",
        "                    train_img_set, train_trnsfrm_set, train_pt_cld_set, bgd_img_set,\n",
        "                    subset_idx, subset_size, transferLearning=True)\n",
        "    \n",
        "    # ID_mask_setset.append(ID_mask_set)\n",
        "    # U_mask_setset.append(U_mask_set)\n",
        "    # V_mask_setset.append(V_mask_set)\n",
        "    # changed_bgd_setset.append(changed_bgd_set)\n",
        "'''\n",
        "'''\n",
        "train_img_indices = load_obj(dataset_dir + \"train_images_indices_transfer\")\n",
        "num_train_imgs = len(train_img_indices)\n",
        "num_subset = 1\n",
        "subset_size = int(num_train_imgs / num_subset)\n",
        "for subset_idx in range(num_subset):\n",
        "    print(\"subset_idx: \", subset_idx)\n",
        "    print(\"subset_size: \", subset_size)\n",
        "    ID_mask_set, U_mask_set, V_mask_set, changed_bgd_set = create_end_arrays(root_dir, dataset_dir, subset_idx, subset_size, transferLearning=True)\n",
        "\n",
        "    save_obj(ID_mask_set, dataset_dir + \"ID_mask_set_small_transfer_\" + str(subset_idx))\n",
        "    save_obj(U_mask_set, dataset_dir + \"U_mask_set_small_transfer_\" + str(subset_idx))\n",
        "    save_obj(V_mask_set, dataset_dir + \"V_mask_set_small_transfer_\" + str(subset_idx))\n",
        "    save_obj(changed_bgd_set, dataset_dir + \"changed_bgd_set_small_transfer_\" + str(subset_idx))\n",
        "    print(\"1\")\n",
        "    np.save( root_dir + \"ID_mask_set_small_transfer_\" + str(subset_idx), ID_mask_set)\n",
        "    print(\"2\")\n",
        "    np.save( root_dir + \"U_mask_set_small_transfer_\" + str(subset_idx), U_mask_set)\n",
        "    print(\"3\")\n",
        "    np.save( root_dir + \"V_mask_set_small_transfer_\" + str(subset_idx), V_mask_set)\n",
        "    print(\"4\")\n",
        "    np.save( root_dir + \"changed_bgd_set_small_transfer_\" + str(subset_idx), changed_bgd_set)\n",
        "'''\n",
        "# ID_mask_set = load_obj(root_dir + \"ID_mask_set\")\n",
        "# U_mask_set = load_obj(root_dir + \"U_mask_set\")\n",
        "# V_mask_set = load_obj(root_dir + \"V_mask_set\")\n",
        "\n",
        "# create_GT_masks(dataset_dir, background_dir, intrnsc_mtrx, classes)\n",
        "create_UV_XYZ_dictionary(dataset_dir, class_list)  # create UV - XYZ dictionaries\n",
        "# print(\"----- Finished creating ground truth -----\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(class_list)"
      ],
      "metadata": {
        "id": "es5ozDDQ8Kco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J33Kg3rIGaf6"
      },
      "outputs": [],
      "source": [
        "# torch.cuda.empty_cache()\n",
        "# print(\"------ Started training of the correspondence block ------\")\n",
        "# train_correspondence_block(root_dir, dataset_dir, model_dir, classes, epochs=10, transferLearning=True)\n",
        "# print(\"------ Training Finished ------\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcs_WTNLGdBC"
      },
      "outputs": [],
      "source": [
        "# print(\"------ Started Initial pose estimation ------\")\n",
        "# initial_pose_estimation(dataset_dir, model_dir, classes, intrnsc_mtrx, transferLearning=True)\n",
        "# print(\"------ Finished Initial pose estimation -----\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vI91flAGevT"
      },
      "outputs": [],
      "source": [
        "# pred_poses_dict = create_pred_pose_dataset(root_dir, dataset_dir, classes, transferLearning=True)\n",
        "# save_obj(pred_poses_dict, dataset_dir + \"pred_poses_dict_transfer\")\n",
        "\n",
        "# print(\"----- Started creating inputs for DL based pose refinement ------\")\n",
        "# create_refinement_inputs(root_dir, dataset_dir, model_dir, classes, intrnsc_mtrx, transferLearning=True)\n",
        "# print(\"----- Finished creating inputs for DL based pose refinement\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EJGwgRjGgK-"
      },
      "outputs": [],
      "source": [
        "# train_img_indices = load_obj(dataset_dir + \"train_images_indices_transfer\")\n",
        "# num_train_imgs = len(train_img_indices)\n",
        "# num_subset = 1\n",
        "# subset_size = int(num_train_imgs / num_subset)\n",
        "# for subset_idx in range(num_subset):\n",
        "#     print(\"subset_idx: \", subset_idx)\n",
        "#     print(\"subset_size: \", subset_size)\n",
        "#     real_set, rendered_set = create_end_arrays_2(root_dir, dataset_dir, subset_idx, subset_size, transferLearning=True)\n",
        "\n",
        "#     save_obj(real_set, dataset_dir + \"real_set_transfer_\" + str(subset_idx))\n",
        "#     save_obj(rendered_set, dataset_dir + \"rendered_set_transfer_\" + str(subset_idx))\n",
        "#     print(\"1\")\n",
        "#     np.save( root_dir + \"real_set_transfer_\" + str(subset_idx), np.array(real_set, dtype='object'))\n",
        "#     print(\"2\")\n",
        "#     np.save( root_dir + \"rendered_set_transfer_\" + str(subset_idx), np.array(rendered_set, dtype='object'))\n",
        "\n",
        "\n",
        "# print(\"----- Started training DL based pose refiner ------\")\n",
        "# train_pose_refinement(root_dir, dataset_dir, model_dir, classes, epochs=10, transferLearning=True)\n",
        "# print(\"----- Finished training DL based pose refiner ------\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### wrap up trained data back to home directory\n",
        "# !zip -r -q \"./Dataset/ape__.zip\" \"./Dataset/ape/\"\n",
        "# !zip -r -q \"./Dataset/benchviseblue__.zip\" \"./Dataset/benchviseblue/\"\n",
        "# !zip -r -q \"./Dataset/cam__.zip\" \"./Dataset/cam/\"\n",
        "# !zip -r -q \"./Dataset/can__.zip\" \"./Dataset/can/\"\n",
        "# !zip -r -q \"./Dataset/cat__.zip\" \"./Dataset/cat/\"\n",
        "# !zip -r -q \"./Dataset/driller__.zip\" \"./Dataset/driller/\"\n",
        "# !zip -r -q \"./Dataset/duck__.zip\" \"./Dataset/duck/\"\n",
        "# !zip -r -q \"./Dataset/eggbox__.zip\" \"./Dataset/eggbox/\"\n",
        "# !zip -r -q \"./Dataset/glue__.zip\" \"./Dataset/glue/\"\n",
        "# !zip -r -q \"./Dataset/holepuncher__.zip\" \"./Dataset/holepuncher/\"\n",
        "# !zip -r -q \"./Dataset/iron__.zip\" \"./Dataset/iron/\"\n",
        "# !zip -r -q \"./Dataset/lamp__.zip\" \"./Dataset/lamp/\"\n",
        "# !zip -r -q \"./Dataset/phone__.zip\" \"./Dataset/phone/\"\n",
        "# !zip -r -q \"./Dataset/TransferLearning/blueduck2500.zip\" \"./Dataset/TransferLearning/duck/\"\n",
        "\n",
        "\n",
        "# !cp \"./Dataset/ape__.zip\" \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/\"\n",
        "# !cp \"./Dataset/benchviseblue__.zip\" \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/\"\n",
        "# !cp \"./Dataset/cam__.zip\" \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/\"\n",
        "# !cp \"./Dataset/can__.zip\" \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/\"\n",
        "# !cp \"./Dataset/cat__.zip\" \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/\"\n",
        "# !cp \"./Dataset/driller__.zip\" \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/\"\n",
        "# !cp \"./Dataset/duck__.zip\" \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/\"\n",
        "# !cp \"./Dataset/eggbox__.zip\" \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/\"\n",
        "# !cp \"./Dataset/glue__.zip\" \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/\"\n",
        "# !cp \"./Dataset/holepuncher__.zip\" \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/\"\n",
        "# !cp \"./Dataset/iron__.zip\" \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/\"\n",
        "# !cp \"./Dataset/lamp__.zip\" \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/\"\n",
        "# !cp \"./Dataset/phone__.zip\" \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/\"\n",
        "# !cp \"./Dataset/TransferLearning/blueduck2500.zip\" \"/content/gdrive/MyDrive/path/of/folder/where/you/put/the/dataset/\"\n"
      ],
      "metadata": {
        "id": "qz90i64nyZNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####################\n",
        "###### eval.py ######\n",
        "#####################\n",
        "\n",
        "import os\n",
        "import re\n",
        "import cv2\n",
        "import torch\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import unet_model as UNET\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# from helper import load_obj, ADD_score, save_obj\n",
        "from torchvision import transforms, utils\n",
        "# from create_ground_truth import get_rot_tra\n",
        "from scipy.spatial.transform import Rotation as R\n",
        "# from pose_refiner_architecture import Pose_Refiner\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "def create_rendering_old(dataset_dir, intrinsic_matrix, obj, rigid_transformation):\n",
        "    # helper function to help with creating renderings\n",
        "    rgb_values = np.loadtxt(dataset_dir + obj + '/object.xyz',\n",
        "                            skiprows=1, usecols=(6, 7, 8))\n",
        "    coords_3d = np.loadtxt(dataset_dir + obj + '/object.xyz',\n",
        "                           skiprows=1, usecols=(0, 1, 2))\n",
        "    ones = np.ones((coords_3d.shape[0], 1))\n",
        "    homogenous_coordinate = np.append(coords_3d, ones, axis=1)\n",
        "    # Perspective Projection to obtain 2D coordinates\n",
        "    homogenous_2D = intrinsic_matrix @ (\n",
        "        rigid_transformation @ homogenous_coordinate.T)\n",
        "    homogenous_2D[2, :][np.where(homogenous_2D[2, :] == 0)] = 1\n",
        "    coord_2D = homogenous_2D[:2, :] / homogenous_2D[2, :]\n",
        "    coord_2D = ((np.floor(coord_2D)).T).astype(int)\n",
        "    rendered_img = np.zeros((480, 640, 3))\n",
        "    x_2d = np.clip(coord_2D[:, 0], 0, 479)\n",
        "    y_2d = np.clip(coord_2D[:, 1], 0, 639)\n",
        "    rendered_img[x_2d, y_2d, :] = rgb_values\n",
        "    temp = np.sum(rendered_img, axis=2)\n",
        "    non_zero_indices = np.argwhere(temp > 0)\n",
        "    min_x = non_zero_indices[:, 0].min()\n",
        "    max_x = non_zero_indices[:, 0].max()\n",
        "    min_y = non_zero_indices[:, 1].min()\n",
        "    max_y = non_zero_indices[:, 1].max()\n",
        "    cropped_rendered_img = rendered_img[min_x:max_x +\n",
        "                                        1, min_y:max_y + 1, :]\n",
        "    if cropped_rendered_img.shape[0] > 240 or cropped_rendered_img.shape[1] > 320:\n",
        "        cropped_rendered_img = cv2.resize(np.float32(\n",
        "            cropped_rendered_img), (320, 240), interpolation=cv2.INTER_AREA)\n",
        "    return cropped_rendered_img\n",
        "\n",
        "\n",
        "# parser = argparse.ArgumentParser(\n",
        "#     description='Script to create the Ground Truth masks')\n",
        "# parser.add_argument(\"--root_dir\", default=\"/home/jovyan/work/LineMOD_Dataset/\",\n",
        "#                     help=\"path to dataset directory\")\n",
        "# args = parser.parse_args()\n",
        "\n",
        "# root_dir = args.root_dir\n",
        "\n",
        "classes = {'ape': 1, 'benchviseblue': 2, 'cam': 3, 'can': 4, 'cat': 5, 'driller': 6,\n",
        "           'duck': 7, 'eggbox': 8, 'glue': 9, 'holepuncher': 10, 'iron': 11, 'lamp': 12,\n",
        "           'phone': 13}\n",
        "\n",
        "score_card = {'ape': 0, 'benchviseblue': 0, 'cam': 0, 'can': 0, 'cat': 0, 'driller': 0,\n",
        "              'duck': 0, 'eggbox': 0, 'glue': 0, 'holepuncher': 0, 'iron': 0, 'lamp': 0, 'phone': 0}\n",
        "\n",
        "instances = {'ape': 0, 'benchviseblue': 0, 'cam': 0, 'can': 0, 'cat': 0, 'driller': 0,\n",
        "             'duck': 0, 'eggbox': 0, 'glue': 0, 'holepuncher': 0, 'iron': 0, 'lamp': 0, 'phone': 0}\n",
        "\n",
        "transform = transforms.Compose([\n",
        "                                transforms.ToPILImage(mode=None),\n",
        "                                transforms.Resize(size=(224, 224)),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "# Intrinsic Parameters of the Camera\n",
        "fx = 572.41140\n",
        "px = 325.26110\n",
        "fy = 573.57043\n",
        "py = 242.04899\n",
        "intrinsic_matrix = np.array([[fx, 0, px], [0, fy, py], [0, 0, 1]])\n",
        "\n",
        "# test_start = 784\n",
        "test_start = 0\n",
        "show_drtn = 100\n",
        "test_blueduck = True\n",
        "transferLearning = True\n",
        "useMask = True\n",
        "\n",
        "if (test_blueduck == False):\n",
        "    list_all_images = load_obj(dataset_dir + \"all_images_adr_test\")\n",
        "    testing_images_idx = load_obj(dataset_dir + \"test_images_indices_test\")\n",
        "\n",
        "else:\n",
        "    pass\n",
        "    # list_all_images = load_obj(dataset_dir + \"all_images_adr_test\")\n",
        "    # testing_images_idx = load_obj(dataset_dir + \"train_images_indices_test\") + load_obj(dataset_dir + \"test_images_indices_test\")\n",
        "\n",
        "num_test = 100\n",
        "# num_test = 1254\n",
        "# num_test = len(testing_images_idx)\n",
        "\n",
        "correspondence_block = UNET.UNet(n_channels=3, out_channels_id=14, out_channels_uv=256, bilinear=True)\n",
        "\n",
        "# load the best weights from the training loop\n",
        "if (transferLearning):\n",
        "    correspondence_block.load_state_dict(torch.load(model_dir + 'correspondence_block_transfer.pt', map_location=torch.device('cpu')))\n",
        "else:\n",
        "    correspondence_block.load_state_dict(torch.load(model_dir + 'correspondence_block.pt', map_location=torch.device('cpu')))\n",
        "\n",
        "pose_refiner = Pose_Refiner()\n",
        "\n",
        "# load the best weights from the training loop\n",
        "pose_refiner.load_state_dict(torch.load(model_dir + 'pose_refiner.pt', map_location=torch.device('cpu')))\n",
        "\n",
        "correspondence_block.cuda()\n",
        "pose_refiner.cuda()\n",
        "pose_refiner.eval()\n",
        "correspondence_block.eval()\n",
        "\n",
        "regex = re.compile(r'\\d+')\n",
        "upsampled = nn.Upsample(size=[240, 320], mode='bilinear', align_corners=False)\n",
        "total_score = 0\n",
        "start_time = time.time()\n",
        "for i in range(test_start, test_start + num_test):\n",
        "# for i in range(len(testing_images_idx)):\n",
        "    if (test_blueduck == False):\n",
        "        # img_adr = list_all_images[testing_images_idx[i]]\n",
        "        img_adr = \"/content/Dataset/holepuncher/data/color\" + str(i) + \".jpg\"\n",
        "    \n",
        "    else:\n",
        "        img_adr = \"/content/Dataset/duck/data/color\" + str(i) + \".png\" # for lm duck\n",
        "        if (useMask):\n",
        "            mask_adr = \"/content/Dataset/duck/mask/\" + format(i, \"06\") + \".png\" # for blue duck\n",
        "        # img_adr = list_all_images[testing_images_idx[i]]\n",
        "\n",
        "    label = os.path.split(os.path.split(os.path.dirname(img_adr))[0])[1]\n",
        "    \n",
        "    if (test_blueduck == True):\n",
        "        if (label != 'duck'):\n",
        "            continue\n",
        "\n",
        "    idx = regex.findall(os.path.split(img_adr)[1])[0]\n",
        "\n",
        "    tra_adr = dataset_dir + label + \"/data/tra\" + str(idx) + \".tra\"\n",
        "    rot_adr = dataset_dir + label + \"/data/rot\" + str(idx) + \".rot\"\n",
        "    true_pose = get_rot_tra(rot_adr, tra_adr)\n",
        "\n",
        "    test_img = cv2.imread(img_adr)\n",
        "    test_img = cv2.resize(test_img, (test_img.shape[1]//2, test_img.shape[0]//2), interpolation=cv2.INTER_AREA)\n",
        "    test_img_ = test_img\n",
        "    test_img = torch.from_numpy(test_img).type(torch.double)\n",
        "    test_img = test_img.transpose(1, 2).transpose(0, 1)\n",
        "\n",
        "    if (test_blueduck and useMask):\n",
        "        ### for blue duck\n",
        "        test_mask = cv2.imread(mask_adr, cv2.IMREAD_GRAYSCALE)\n",
        "        test_mask = 255 - test_mask\n",
        "        test_mask = cv2.resize(test_mask, (test_mask.shape[1]//2, test_mask.shape[0]//2), interpolation=cv2.INTER_AREA)\n",
        "        test_mask = torch.from_numpy(test_mask).type(torch.double)\n",
        "\n",
        "    if (len(test_img.shape) != 4):\n",
        "        test_img = test_img.view(1, test_img.shape[0], test_img.shape[1], test_img.shape[2])\n",
        "\n",
        "    # pass through correspondence block\n",
        "    idmask_pred, umask_pred, vmask_pred = correspondence_block(test_img.float().cuda())\n",
        "\n",
        "    # convert the masks to (240, 320) shape\n",
        "    temp = torch.argmax(idmask_pred, dim=1).squeeze().cpu()\n",
        "    upred = torch.argmax(umask_pred, dim=1).squeeze().cpu()\n",
        "    vpred = torch.argmax(vmask_pred, dim=1).squeeze().cpu()\n",
        "    if (test_blueduck == True):  \n",
        "        if (label =='lmduck'):\n",
        "            coord_2d = (temp == classes['duck']).nonzero(as_tuple=True) # for LM test duck\n",
        "        else:\n",
        "            if (useMask):\n",
        "                coord_2d = test_mask.nonzero(as_tuple=True) # for blue duck\n",
        "            else:\n",
        "                coord_2d = (temp == classes[label]).nonzero(as_tuple=True) # for yellow duck\n",
        "            \n",
        "    else:\n",
        "        coord_2d = (temp == classes[label]).nonzero(as_tuple=True)\n",
        "    \n",
        "    if (coord_2d[0].nelement() != 0):  # label is detected in the image\n",
        "        coord_2d = torch.cat((coord_2d[0].view(\n",
        "            coord_2d[0].shape[0], 1), coord_2d[1].view(coord_2d[1].shape[0], 1)), 1)\n",
        "        uvalues = upred[coord_2d[:, 0], coord_2d[:, 1]]\n",
        "        vvalues = vpred[coord_2d[:, 0], coord_2d[:, 1]]\n",
        "        dct_keys = torch.cat((uvalues.view(-1, 1), vvalues.view(-1, 1)), 1)\n",
        "        dct_keys = tuple(dct_keys.numpy())\n",
        "        dct = load_obj(dataset_dir + label + \"/UV-XYZ_mapping\")\n",
        "        mapping_2d = []\n",
        "        mapping_3d = []\n",
        "        for count, (u, v) in enumerate(dct_keys):\n",
        "            if (u, v) in dct:\n",
        "                mapping_2d.append(np.array(coord_2d[count]))\n",
        "                mapping_3d.append(dct[(u, v)])\n",
        "\n",
        "        # PnP needs atleast 6 unique 2D-3D correspondences to run\n",
        "        if len(mapping_2d) >= 6 or len(mapping_3d) >= 6:\n",
        "            _, rvecs, tvecs, inliers = cv2.solvePnPRansac(np.array(mapping_3d, dtype=np.float32),\n",
        "                                                          np.array(mapping_2d, dtype=np.float32), intrinsic_matrix, distCoeffs=None,\n",
        "                                                          iterationsCount=150, reprojectionError=1.0, flags=cv2.SOLVEPNP_P3P)\n",
        "            rot, _ = cv2.Rodrigues(rvecs, jacobian=None)\n",
        "            pred_pose = np.append(rot, tvecs, axis=1)\n",
        "\n",
        "        else:  # save an empty file\n",
        "            pred_pose = np.zeros((3, 4))\n",
        "\n",
        "        min_x = coord_2d[:, 0].min()\n",
        "        max_x = coord_2d[:, 0].max()\n",
        "        min_y = coord_2d[:, 1].min()\n",
        "        max_y = coord_2d[:, 1].max()\n",
        "        img = test_img.squeeze().transpose(1, 2).transpose(0, 2)\n",
        "        obj_img = img[min_x:max_x+1, min_y:max_y+1, :]\n",
        "        \n",
        "        # saving in the correct format using upsampling\n",
        "        obj_img = obj_img.transpose(0, 1).transpose(0, 2).unsqueeze(dim=0)\n",
        "        obj_img = upsampled(obj_img)\n",
        "        obj_img = obj_img.squeeze()\n",
        "        # obj_img = obj_img.squeeze().transpose(0, 2).transpose(0, 1)\n",
        "        obj_img = transform(torch.as_tensor(obj_img, dtype=torch.float32))\n",
        "        \n",
        "        # create rendering for an object\n",
        "        cropped_rendered_img = create_rendering_old(dataset_dir, intrinsic_matrix, label, pred_pose)\n",
        "        rendered_img = torch.from_numpy(cropped_rendered_img)\n",
        "        rendered_img = rendered_img.unsqueeze(dim=0)\n",
        "        rendered_img = rendered_img.transpose(1, 3).transpose(2, 3)\n",
        "        rendered_img = upsampled(rendered_img)\n",
        "        rendered_img = rendered_img.squeeze()\n",
        "        rendered_img = transform(torch.as_tensor(\n",
        "            rendered_img, dtype=torch.float32))\n",
        "\n",
        "        if len(rendered_img.shape) != 4:\n",
        "            rendered_img = rendered_img.view(\n",
        "                1, rendered_img.shape[0], rendered_img.shape[1], rendered_img.shape[2])\n",
        "\n",
        "        if len(obj_img.shape) != 4:\n",
        "            obj_img = obj_img.view(\n",
        "                1, obj_img.shape[0], obj_img.shape[1],  obj_img.shape[2])\n",
        "        pred_pose = (torch.from_numpy(pred_pose)).unsqueeze(0)\n",
        "\n",
        "        # pose refinement to get final output\n",
        "        xy, z, rot = pose_refiner(obj_img.float().cuda(),\n",
        "                                  rendered_img.float().cuda(), pred_pose)\n",
        "        \n",
        "        # below 2 lines are for outliers only - edge case                          \n",
        "        rot[torch.isnan(rot)] = 1  # take care of NaN and inf values\n",
        "        rot[rot == float(\"Inf\")] = 1\n",
        "        xy[torch.isnan(xy)] = 0\n",
        "        z[torch.isnan(z)] = 0\n",
        "        \n",
        "        # convert R quarternion to rotational matrix\n",
        "        rot = (R.from_quat(rot.detach().cpu().numpy())).as_matrix()\n",
        "        pred_pose = pred_pose.squeeze().numpy()\n",
        "        # print(pred_pose)\n",
        "\n",
        "        # update predicted pose\n",
        "        xy = xy.squeeze()\n",
        "        pred_pose[0:3, 0:3] = rot\n",
        "        pred_pose[0, 3] = xy[0]\n",
        "        pred_pose[1, 3] = xy[1]\n",
        "        pred_pose[2, 3] = z\n",
        "\n",
        "        diameter = np.loadtxt(dataset_dir + label + \"/distance.txt\")\n",
        "        ptcld_file = dataset_dir + label + \"/object.xyz\"\n",
        "        pt_cld = np.loadtxt(ptcld_file, skiprows=1, usecols=(0, 1, 2))\n",
        "        # score = ADD_correct(pt_cld, true_pose, pred_pose, diameter)\n",
        "        score = ADD_S_correct(pt_cld, true_pose, pred_pose, diameter)\n",
        "        total_score += score\n",
        "        if (label =='lmduck'):\n",
        "            score_card['duck'] += score\n",
        "        else:\n",
        "            score_card[label] += score\n",
        "\n",
        "        # vis_img = create_bounding_box(test_img_, pred_pose, pt_cld, intrinsic_matrix)\n",
        "        # plt.imshow(vis_img)\n",
        "        # plt.show()\n",
        "\n",
        "    else:\n",
        "        if (label =='lmduck'):\n",
        "            score_card['duck'] += score\n",
        "        else:\n",
        "            score_card[label] += 0\n",
        "    \n",
        "    if (label =='lmduck'):\n",
        "        instances['duck'] += 1\n",
        "    else:\n",
        "        instances[label] += 1\n",
        "\n",
        "    if(i % show_drtn == 0):\n",
        "        print(str(i - test_start) + \"/\" + str(num_test) +\n",
        "                  \" finished! {:2.2f}\".format(time.time() - start_time), \" sec, \", \"score: {:2.4f}\".format(total_score/(i - test_start + 1e-6)))\n",
        "        start_time = time.time()\n",
        "        \n",
        "\n",
        "print(\"ADD Score for all testing images is: \", total_score/num_test)\n"
      ],
      "metadata": {
        "id": "cb4fUnIWBxcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(instances)\n",
        "print(score_card)"
      ],
      "metadata": {
        "id": "p5lwMMowcxZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.imshow(temp.view(temp.shape[0], temp.shape[1]))\n",
        "# plt.imshow(test_mask.view(temp.shape[0], temp.shape[1]))"
      ],
      "metadata": {
        "id": "x5FOkm_W7otU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_img_set_orgn = load_obj(dataset_dir + \"train_img_set\")\n",
        "# bgd_img_set_orgn = load_obj(root_dir + \"bgd_img_set\")\n",
        "# num_subset = 5\n",
        "# for subset_idx in range(num_subset):\n",
        "#     print(subset_idx)\n",
        "#     train_img_subset = load_obj(root_dir + \"train_img_set_\" + str(subset_idx))\n",
        "#     changed_bgd_subset = load_obj(root_dir + \"changed_bgd_set_\" + str(subset_idx))\n",
        "#     subset_size = len(train_img_subset)\n",
        "#     print(subset_size)\n",
        "#     for train_img_idx in range(subset_idx * subset_size, (subset_idx + 1) * subset_size):\n",
        "#         if (train_img_idx % 100 == 0):\n",
        "#             changed_bgd_subset[train_img_idx - subset_idx * subset_size] = train_img_subset[train_img_idx - subset_idx * subset_size]\n",
        "#     save_obj(changed_bgd_subset, root_dir + \"changed_bgd_set_\" + str(subset_idx))\n",
        "#     np.save( root_dir + \"changed_bgd_set_\" + str(subset_idx), np.array(changed_bgd_set, dtype='object'))\n"
      ],
      "metadata": {
        "id": "a2HboFh4F5xL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for subset_idx in range(5):\n",
        "#     print(subset_idx)\n",
        "#     changed_bgd_subset = load_obj(dataset_dir + \"changed_bgd_set_\"+ str(subset_idx))\n",
        "#     new_changed_bgd_subset = []\n",
        "#     for image_idx in range(len(changed_bgd_subset)):\n",
        "#         image = changed_bgd_subset[image_idx]\n",
        "        \n",
        "#         if(image_idx % 50 == 0):\n",
        "#             print(image_idx)\n",
        "#             print(image.shape)\n",
        "#         new_changed_bgd_subset.append( cv2.resize(image, (image.shape[1]//2, image.shape[0]//2), interpolation=cv2.INTER_AREA) )\n",
        "#     save_obj( new_changed_bgd_subset, root_dir + \"changed_bgd_set_small_\"+ str(subset_idx) )        \n",
        "#     save_obj( new_changed_bgd_subset, dataset_dir + \"changed_bgd_set_small_\"+ str(subset_idx) )        \n",
        "    # np.save( root_dir + \"changed_bgd_set_small_\"+ str(subset_idx), np.array(new_changed_bgd_subset, dtype='object'))\n",
        "\n",
        "# for subset_idx in range(5):\n",
        "#     print(subset_idx)\n",
        "#     ID_mask_subset = load_obj(dataset_dir + \"ID_mask_set_\"+ str(subset_idx))\n",
        "#     new_ID_mask_subset = []\n",
        "#     for ID_mask_idx in range(len(ID_mask_subset)):\n",
        "#         ID_mask = ID_mask_subset[ID_mask_idx]\n",
        "        \n",
        "#         if(ID_mask_idx % 50 == 0):\n",
        "#             print(ID_mask_idx)\n",
        "#             print(ID_mask.shape)\n",
        "    \n",
        "#         new_ID_mask_subset.append( cv2.resize(ID_mask, (ID_mask.shape[1]//2, ID_mask.shape[0]//2), interpolation=cv2.INTER_AREA) )\n",
        "    \n",
        "#     save_obj( new_ID_mask_subset, root_dir + \"ID_mask_set_small_\"+ str(subset_idx) )        \n",
        "#     save_obj( new_ID_mask_subset, dataset_dir + \"ID_mask_set_small_\"+ str(subset_idx) )        \n",
        "    # np.save( root_dir + \"changed_bgd_set_small_\"+ str(subset_idx), np.array(new_changed_bgd_subset, dtype='object'))\n",
        "\n",
        "# for subset_idx in range(5):\n",
        "#     print(subset_idx)\n",
        "#     U_mask_subset = load_obj(dataset_dir + \"U_mask_set_\"+ str(subset_idx))\n",
        "#     new_U_mask_subset = []\n",
        "#     for U_mask_idx in range(len(U_mask_subset)):\n",
        "#         U_mask = U_mask_subset[U_mask_idx]\n",
        "        \n",
        "#         if(U_mask_idx % 50 == 0):\n",
        "#             print(U_mask_idx)\n",
        "#             print(U_mask.shape)\n",
        "    \n",
        "#         new_U_mask_subset.append( cv2.resize(U_mask, (U_mask.shape[1]//2, U_mask.shape[0]//2), interpolation=cv2.INTER_AREA) )\n",
        "\n",
        "#     save_obj( new_U_mask_subset, root_dir + \"U_mask_set_small_\"+ str(subset_idx) )        \n",
        "#     save_obj( new_U_mask_subset, dataset_dir + \"U_mask_set_small_\"+ str(subset_idx) )        \n",
        "    # np.save( root_dir + \"changed_bgd_set_small_\"+ str(subset_idx), np.array(new_changed_bgd_subset, dtype='object'))\n",
        "\n",
        "# for subset_idx in range(5):\n",
        "#     print(subset_idx)\n",
        "#     V_mask_subset = load_obj(dataset_dir + \"V_mask_set_\"+ str(subset_idx))\n",
        "#     new_V_mask_subset = []\n",
        "#     for V_mask_idx in range(len(V_mask_subset)):\n",
        "#         V_mask = V_mask_subset[V_mask_idx]\n",
        "        \n",
        "#         if(V_mask_idx % 50 == 0):\n",
        "#             print(V_mask_idx)\n",
        "#             print(V_mask.shape)\n",
        "    \n",
        "#         new_V_mask_subset.append( cv2.resize(V_mask, (V_mask.shape[1]//2, V_mask.shape[0]//2), interpolation=cv2.INTER_AREA) )\n",
        "\n",
        "#     save_obj( new_V_mask_subset, root_dir + \"V_mask_set_small_\"+ str(subset_idx) )        \n",
        "#     save_obj( new_V_mask_subset, dataset_dir + \"V_mask_set_small_\"+ str(subset_idx) )        \n",
        "    # np.save( root_dir + \"changed_bgd_set_small_\"+ str(subset_idx), np.array(new_changed_bgd_subset, dtype='object'))\n",
        "\n"
      ],
      "metadata": {
        "id": "FXNVFHE_p59j"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of main.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}