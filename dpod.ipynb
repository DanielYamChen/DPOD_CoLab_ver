{"cells":[{"cell_type":"markdown","source":"# [Dense Pose Object Detector](https://arxiv.org/abs/1902.11020)","metadata":{"tags":[],"cell_id":"ebcfae02-8a28-41c3-9533-4713e2bc15a0"}},{"cell_type":"markdown","source":"#### Download and unzip the Linemod Dataset \n","metadata":{"tags":[],"output_cleared":true,"cell_id":"ff15665c-a501-42ea-9047-94a006cabbe3"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"30df4087-0cde-43a7-97c6-90279a104f88"},"source":"# !wget http://campar.in.tum.de/personal/hinterst/index/downloads!09384230443!/ape.zip\n# !wget http://campar.in.tum.de/personal/hinterst/index/downloads!09384230443!/benchviseblue.zip\n# !wget http://campar.in.tum.de/personal/hinterst/index/downloads!09384230443!/bowl.zip\n# !wget http://campar.in.tum.de/personal/hinterst/index/downloads!09384230443!/can.zip\n# !wget http://campar.in.tum.de/personal/hinterst/index/downloads!09384230443!/cat.zip\n# !wget http://campar.in.tum.de/personal/hinterst/index/downloads!09384230443!/cup.zip\n# !wget http://campar.in.tum.de/personal/hinterst/index/downloads!09384230443!/driller.zip\n# !wget http://campar.in.tum.de/personal/hinterst/index/downloads!09384230443!/duck.zip\n# !wget http://campar.in.tum.de/personal/hinterst/index/downloads!09384230443!/glue.zip\n# !wget http://campar.in.tum.de/personal/hinterst/index/downloads!09384230443!/holepuncher.zip\n# !wget http://campar.in.tum.de/personal/hinterst/index/downloads!09384230443!/iron.zip\n# !wget http://campar.in.tum.de/personal/hinterst/index/downloads!09384230443!/lamp.zip\n# !wget http://campar.in.tum.de/personal/hinterst/index/downloads!09384230443!/phone.zip\n# !wget http://campar.in.tum.de/personal/hinterst/index/downloads!09384230443!/cam.zip\n# !wget http://campar.in.tum.de/personal/hinterst/index/downloads!09384230443!/eggbox.zip\n\n# !unzip /home/jovyan/work/ape.zip\n# !unzip /home/jovyan/work/benchviseblue.zip\n# !unzip /home/jovyan/work/bowl.zip\n# !unzip /home/jovyan/work/can.zip\n# !unzip /home/jovyan/work/cat.zip\n# !unzip /home/jovyan/work/cup.zip\n# !unzip /home/jovyan/work/driller.zip\n# !unzip /home/jovyan/work/duck.zip\n# !unzip /home/jovyan/work/glue.zip\n# !unzip /home/jovyan/work/holepuncher.zip\n# !unzip /home/jovyan/work/iron.zip\n# !unzip /home/jovyan/work/lamp.zip\n# !unzip /home/jovyan/work/phone.zip\n# !unzip /home/jovyan/work/cam.zip\n# !unzip /home/jovyan/work/eggbox.zip","execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"0439737d-de4d-4fef-98c1-45a34e314d6e"},"source":"!pip install opencv-python","execution_count":2,"outputs":[{"name":"stdout","text":"Collecting opencv-python\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/f0/cfe88d262c67825b20d396c778beca21829da061717c7aaa8b421ae5132e/opencv_python-4.2.0.34-cp37-cp37m-manylinux1_x86_64.whl (28.2MB)\n\u001b[K     |████████████████████████████████| 28.2MB 3.3MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /opt/venv/lib/python3.7/site-packages (from opencv-python) (1.18.1)\nInstalling collected packages: opencv-python\nSuccessfully installed opencv-python-4.2.0.34\n\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"output_cleared":false,"cell_id":"2f422394-a153-44fc-973b-8f6de322bdd9"},"source":"import numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport cv2\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport unet_model as UNET\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom create_ground_truth_helper import *\nfrom helper import load_obj\n%load_ext autoreload\n%autoreload 2","execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Custom Train Dataset ","metadata":{"tags":[],"cell_id":"005d769c-57d7-4a4c-8063-d91f8da15205"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"944569ae-cd83-466c-9d61-b41bd5fa20d9"},"source":"class LineMODDataset(Dataset):\n\n    \"\"\"\n    Args:\n        root_dir (str): path to the dataset\n        classes (dictionary): values of classes to extract from segmentation mask \n        transform : Transforms for input image\n            \"\"\"\n\n    def __init__(self,root_dir,classes=None,transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.classes = classes\n        self.list_all_images = load_obj(root_dir + \"all_images_adr\")\n        self.training_images_idx = load_obj(root_dir + \"train_images_indices\")\n\n    def __len__(self):\n        return len(self.training_images_idx)\n    def __getitem__(self, i):\n        img_adr = self.list_all_images[self.training_images_idx[i]]\n        label = os.path.split(os.path.split(os.path.dirname(img_adr))[0])[1]\n        regex = re.compile(r'\\d+')\n        idx = regex.findall(os.path.split(img_adr)[1])[0]\n        image = cv2.imread(img_adr) \n        IDmask = cv2.imread(self.root_dir + label + \"/ground_truth/IDmasks/color\" + str(idx) + \".png\"\n                                ,cv2.IMREAD_GRAYSCALE)\n        Umask = cv2.imread(self.root_dir + label + \"/ground_truth/Umasks/color\" + str(idx) + \".png\"\n                                ,cv2.IMREAD_GRAYSCALE)\n        Vmask = cv2.imread(self.root_dir + label + \"/ground_truth/Vmasks/color\" + str(idx) + \".png\"\n                                ,cv2.IMREAD_GRAYSCALE)\n        # resize the masks\n        IDmask = (IDmask/255)*self.classes[label] \n        image = cv2.resize(image,(image.shape[1]//2, image.shape[0]//2), interpolation=cv2.INTER_AREA)\n        IDmask = cv2.resize(IDmask,(IDmask.shape[1]//2, IDmask.shape[0]//2), interpolation=cv2.INTER_AREA)\n        Umask = cv2.resize(Umask,(Umask.shape[1]//2, Umask.shape[0]//2), interpolation=cv2.INTER_AREA)\n        Vmask = cv2.resize(Vmask,(Vmask.shape[1]//2, Vmask.shape[0]//2), interpolation=cv2.INTER_AREA)\n        if self.transform:\n            image = self.transform(image)\n        IDmask = (torch.from_numpy(IDmask)).type(torch.int64)\n        Umask = (torch.from_numpy(Umask)).type(torch.int64)\n        Vmask = (torch.from_numpy(Vmask)).type(torch.int64)\n        return img_adr,image,IDmask,Umask,Vmask","execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"c49d2555-eefa-44c4-b7c3-249193587734"},"source":"classes = {'ape':1, 'benchviseblue':2, 'bowl':3, 'can':4, 'cat':5, 'cup':6, 'driller':7, \n            'duck':8, 'glue':9, 'holepuncher':10, 'iron':11, 'lamp':12, 'phone':13, 'cam':14,'eggbox':15}","execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"80ed0dac-5c02-47a1-afd4-45d89236c74f"},"source":"root_dir = \"/home/jovyan/work/LineMOD_Dataset/\"\ntrain_data = LineMODDataset(\n    root_dir, \n    classes = classes, \n    transform = transforms.Compose([transforms.ToTensor()])\n)","execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Train and Valid DataLoader","metadata":{"tags":[],"cell_id":"39b4218e-c8ef-4483-b413-0ebfb87c92e2"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"178e4d21-ad89-4e3f-989a-c98b2a4f6842"},"source":"batch_size = 4\nnum_workers = 0\nvalid_size = 0.2\n# obtain training indices that will be used for validation\nnum_train = len(train_data)\nindices = list(range(num_train))\nnp.random.shuffle(indices)\nsplit = int(np.floor(valid_size * num_train))\ntrain_idx, valid_idx = indices[split:], indices[:split]\n\n# define samplers for obtaining training and validation batches\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)\n\n# prepare data loaders (combine dataset and sampler)\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n    sampler=train_sampler, num_workers=num_workers)\nvalid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n    sampler=valid_sampler, num_workers=num_workers)","execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Architecture for Correspondence block","metadata":{"tags":[],"cell_id":"c554926b-07e9-4f4f-85ae-3ef4efae5a5c"}},{"cell_type":"code","metadata":{"tags":[],"output_cleared":false,"cell_id":"9e0f89a5-2650-4fea-aea3-c33ae7f42b41"},"source":"correspondence_block = UNET.UNet(n_channels = 3, out_channels_id = 16, out_channels_uv = 256, bilinear=True)\ncorrespondence_block.cuda()","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"UNet(\n  (inc): DoubleConv(\n    (double_conv): Sequential(\n      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (down1): Down(\n    (maxpool_conv): Sequential(\n      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (1): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n  )\n  (down2): Down(\n    (maxpool_conv): Sequential(\n      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (1): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n  )\n  (down3): Down(\n    (maxpool_conv): Sequential(\n      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (1): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n  )\n  (down4): Down(\n    (maxpool_conv): Sequential(\n      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (1): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n  )\n  (up1_id): Up(\n    (up): Upsample(scale_factor=2.0, mode=bilinear)\n    (conv): DoubleConv(\n      (double_conv): Sequential(\n        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (up2_id): Up(\n    (up): Upsample(scale_factor=2.0, mode=bilinear)\n    (conv): DoubleConv(\n      (double_conv): Sequential(\n        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (up3_id): Up(\n    (up): Upsample(scale_factor=2.0, mode=bilinear)\n    (conv): DoubleConv(\n      (double_conv): Sequential(\n        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (up4_id): Up(\n    (up): Upsample(scale_factor=2.0, mode=bilinear)\n    (conv): DoubleConv(\n      (double_conv): Sequential(\n        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (outc_id): OutConv(\n    (conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n  )\n  (up1_uv): Up(\n    (up): Upsample(scale_factor=2.0, mode=bilinear)\n    (conv): DoubleConv(\n      (double_conv): Sequential(\n        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (up2_uv): Up(\n    (up): Upsample(scale_factor=2.0, mode=bilinear)\n    (conv): DoubleConv(\n      (double_conv): Sequential(\n        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (outc_uv1): OutConv(\n    (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n  )\n  (outc_uv2): OutConv(\n    (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n  )\n  (outc_uv3): OutConv(\n    (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n  )\n  (outc_uv4): OutConv(\n    (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n  )\n  (up3_uv): Upsample(scale_factor=2.0, mode=bilinear)\n  (up4_uv): Upsample(scale_factor=2.0, mode=bilinear)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"Custom Loss Function and Optimizer","metadata":{"tags":[],"cell_id":"1c4f72cc-ed34-4aec-8436-d888908ba509"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"dd5796a3-dcb1-4800-ad49-2de05b1739a5"},"source":"criterion_id = nn.CrossEntropyLoss()\ncriterion_u = nn.CrossEntropyLoss()\ncriterion_v = nn.CrossEntropyLoss()\n\n# specify optimizer\noptimizer = optim.Adam(correspondence_block.parameters(), lr=3e-4,weight_decay=3e-5)","execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Train the Model","metadata":{"tags":[],"cell_id":"a987de43-b26a-4764-80ea-e69145af2136"}},{"cell_type":"code","metadata":{"tags":[],"output_cleared":false,"cell_id":"7afde996-e54d-4489-af64-bb747fddce50"},"source":"# number of epochs to train the model\nn_epochs = 10\n\nvalid_loss_min = np.Inf # track change in validation loss\n\nfor epoch in range(1, n_epochs+1):\n    # keep track of training and validation loss\n    train_loss = 0.0\n    valid_loss = 0.0\n    print(\"------ Epoch \",epoch,\" ---------\")\n    \n    ###################\n    # train the model #\n    ###################\n    correspondence_block.train()\n    for _,image, idmask,umask,vmask in train_loader:\n        # move tensors to GPU if CUDA is available\n        image, idmask,umask,vmask = image.cuda(), idmask.cuda(), umask.cuda(), vmask.cuda()\n        # clear the gradients of all optimized variables\n        optimizer.zero_grad()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        idmask_pred,umask_pred,vmask_pred = correspondence_block(image)       \n        # calculate the batch loss\n        loss_id = criterion_id(idmask_pred, idmask)\n        loss_u = criterion_u(umask_pred, umask)\n        loss_v = criterion_v(vmask_pred, vmask)\n        loss = loss_id + loss_u + loss_v\n        # backward pass: compute gradient of the loss with respect to model parameters\n        loss.backward()\n        # perform a single optimization step (parameter update)\n        optimizer.step()\n        # update training loss\n        train_loss += loss.item()\n\n\n    ######################    \n    # validate the model #\n    ######################\n    correspondence_block.eval()\n    for _,image, idmask,umask,vmask in valid_loader:       \n        # move tensors to GPU if CUDA is available\n        image, idmask,umask,vmask = image.cuda(), idmask.cuda(), umask.cuda(), vmask.cuda()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        idmask_pred,umask_pred,vmask_pred = correspondence_block(image)\n        # calculate the batch loss\n        loss_id = criterion_id(idmask_pred, idmask)\n        loss_u = criterion_u(umask_pred, umask)\n        loss_v = criterion_v(vmask_pred, vmask)\n        loss = loss_id + loss_u + loss_v\n        # update average validation loss \n        valid_loss += loss.item()\n    \n    # calculate average losses\n    train_loss = train_loss/len(train_loader.sampler)\n    valid_loss = valid_loss/len(valid_loader.sampler)\n        \n    # print training/validation statistics \n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n        epoch, train_loss, valid_loss))\n    \n    # save model if validation loss has decreased\n    if valid_loss <= valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(correspondence_block.state_dict(), 'correspondence_block.pt')\n        valid_loss_min = valid_loss\n        ","execution_count":10,"outputs":[{"name":"stdout","text":"------ Epoch  1  ---------\nEpoch: 1 \tTraining Loss: 0.114288 \tValidation Loss: 0.074979\nValidation loss decreased (inf --> 0.074979).  Saving model ...\n------ Epoch  2  ---------\nEpoch: 2 \tTraining Loss: 0.071020 \tValidation Loss: 0.068038\nValidation loss decreased (0.074979 --> 0.068038).  Saving model ...\n------ Epoch  3  ---------\nEpoch: 3 \tTraining Loss: 0.065894 \tValidation Loss: 0.073685\n------ Epoch  4  ---------\nEpoch: 4 \tTraining Loss: 0.063808 \tValidation Loss: 0.066853\nValidation loss decreased (0.068038 --> 0.066853).  Saving model ...\n------ Epoch  5  ---------\nEpoch: 5 \tTraining Loss: 0.061883 \tValidation Loss: 0.061034\nValidation loss decreased (0.066853 --> 0.061034).  Saving model ...\n------ Epoch  6  ---------\nEpoch: 6 \tTraining Loss: 0.060640 \tValidation Loss: 0.066362\n------ Epoch  7  ---------\nEpoch: 7 \tTraining Loss: 0.059254 \tValidation Loss: 0.066238\n------ Epoch  8  ---------\nEpoch: 8 \tTraining Loss: 0.057956 \tValidation Loss: 0.061009\nValidation loss decreased (0.061034 --> 0.061009).  Saving model ...\n------ Epoch  9  ---------\nEpoch: 9 \tTraining Loss: 0.057295 \tValidation Loss: 0.057630\nValidation loss decreased (0.061009 --> 0.057630).  Saving model ...\n------ Epoch  10  ---------\nEpoch: 10 \tTraining Loss: 0.056550 \tValidation Loss: 0.055995\nValidation loss decreased (0.057630 --> 0.055995).  Saving model ...\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Load the model with the lowest validation Loss","metadata":{"tags":[],"cell_id":"89198e8a-cbfd-40b3-bafd-15b6c1ce280d"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"e7bcaa82-18eb-493a-b045-d51fe0b50efb"},"source":"correspondence_block.load_state_dict(torch.load('correspondence_block.pt',map_location=torch.device('cpu')))","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00d4be30-31a9-479a-bffc-983758819c66"},"source":"fx=572.41140; px=325.26110; fy=573.57043; py=242.04899 # Intrinsic Parameters of the Camera\nintrinsic_matrix =  np.array([[fx, 0, px], [0, fy, py], [0, 0, 1]],dtype = \"double\")","execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Pose Block","metadata":{"tags":[],"cell_id":"f56c7c04-a280-4581-84bd-1c42a30f35e2"}},{"cell_type":"code","metadata":{"tags":[],"output_cleared":false,"cell_id":"c2d0ea44-79f9-4b4f-8b7e-1e65a58426a5"},"source":"regex = re.compile(r'\\d+')\nroot_dir = \"/home/jovyan/work/LineMOD_Dataset/\"\nfor i in range(len(train_data)):\n    if i % 1000 == 0:\n        print(str(i) + \"/\" + str(len(train_data)) + \" finished!\")\n    img_adr,img,idmask, _ , _ = train_data[i]\n    label = os.path.split(os.path.split(os.path.dirname(img_adr))[0])[1]\n    idx = regex.findall(os.path.split(img_adr)[1])[0]\n    img = img.view(1,img.shape[0],img.shape[1],img.shape[2])\n    idmask_pred,umask_pred,vmask_pred = correspondence_block(img.cuda())\n    # convert the masks to 240,320 shape\n    temp = torch.argmax(idmask_pred,dim=1).squeeze().cpu() \n    upred = torch.argmax(umask_pred,dim=1).squeeze().cpu()\n    vpred = torch.argmax(vmask_pred,dim=1).squeeze().cpu()\n    coord_2d = (temp == classes[label]).nonzero(as_tuple=True)\n\n    adr = root_dir + label + \"/predicted_pose/\" + \"info_\" + str(idx) + \".txt\"\n\n    coord_2d = torch.cat((coord_2d[0].view(coord_2d[0].shape[0],1),coord_2d[1].view(coord_2d[1].shape[0],1)),1)\n    uvalues = upred[coord_2d[:,0],coord_2d[:,1]]\n    vvalues = vpred[coord_2d[:,0],coord_2d[:,1]]\n    dct_keys = torch.cat((uvalues.view(-1,1),vvalues.view(-1,1)),1)\n    dct_keys = tuple(dct_keys.numpy())\n    dct = load_obj(root_dir + label + \"/UV-XYZ_mapping\")\n    mapping_2d = []\n    mapping_3d = []\n    for count,(u, v) in enumerate(dct_keys):\n        if (u, v) in dct:\n            mapping_2d.append(np.array(coord_2d[count]))\n            mapping_3d.append(dct[(u,v)])\n    # Get the 6D pose from rotation and translation matrices\n    \n    if len(mapping_2d) >= 6 or len(mapping_3d) >= 6: # PnP needs atleast 6 unique 2D-3D correspondences to run\n        _,rvecs, tvecs, inliers = cv2.solvePnPRansac(np.array(mapping_3d,dtype=np.float32),\n            np.array(mapping_2d,dtype = np.float32),intrinsic_matrix,distCoeffs = None,\n                iterationsCount = 150, reprojectionError = 1.0,flags = cv2.SOLVEPNP_P3P)\n        rot, _ = cv2.Rodrigues(rvecs,jacobian = None)\n        rot_tra = np.append(rot,tvecs,axis = 1)\n        # save the predicted pose\n        np.savetxt(adr,rot_tra)\n    else: # save an empty file\n        np.savetxt(adr,np.zeros((3,4)))","execution_count":13,"outputs":[{"name":"stdout","text":"0/14620 finished!\n1000/14620 finished!\n2000/14620 finished!\n3000/14620 finished!\n4000/14620 finished!\n5000/14620 finished!\n6000/14620 finished!\n7000/14620 finished!\n8000/14620 finished!\n9000/14620 finished!\n10000/14620 finished!\n11000/14620 finished!\n12000/14620 finished!\n13000/14620 finished!\n14000/14620 finished!\n","output_type":"stream"}]}],"nbformat":4,"nbformat_minor":2,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"deepnote_execution_queue":[],"deepnote_notebook_id":"ca5bba1f-bfd7-40f1-861b-8e2ad9087c85"}}