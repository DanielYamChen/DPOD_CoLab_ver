{"cells":[{"cell_type":"markdown","source":"# Create data for Deep Model-based Pose refinement","metadata":{"tags":[],"cell_id":"ec7c9573-6e5d-4076-a3aa-92cef31c1d7c"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"eb6d1d8d-d6ff-46d7-a9b1-51a10b4e914d"},"source":"!pip install opencv-python","outputs":[{"name":"stdout","text":"Requirement already satisfied: opencv-python in /opt/venv/lib/python3.7/site-packages (4.2.0.34)\nRequirement already satisfied: numpy>=1.14.5 in /opt/venv/lib/python3.7/site-packages (from opencv-python) (1.18.1)\n\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"8a076704-0b9b-419c-8b79-cc54f101d39d"},"source":"import numpy as np\nimport os\nimport re\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport cv2\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils, models\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport unet_model as UNET\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom create_ground_truth_helper import get_rot_tra\nfrom pose_refiner_block import Pose_Refiner\nfrom scipy.spatial.transform import Rotation as R\nfrom helper import *\n%load_ext autoreload\n%autoreload 2","outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"111ed625-f2b3-4f57-957a-f77c4d1969eb"},"source":"classes = {'ape':1, 'benchviseblue':2, 'bowl':3, 'can':4, 'cat':5, 'cup':6, 'driller':7, \n            'duck':8, 'glue':9, 'holepuncher':10, 'iron':11, 'lamp':12, 'phone':13, 'cam':14,'eggbox':15}","outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"9cc7f06d-353a-4a85-9b35-e576d50669fc"},"source":"correspondence_block = UNET.UNet(n_channels = 3, out_channels_id = 16, out_channels_uv = 256, bilinear=True)\ncorrespondence_block.cuda()","outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"6567d2e0-19a9-43d2-9b91-260743e0d28e"},"source":"correspondence_block.load_state_dict(torch.load('correspondence_block.pt',map_location=torch.device('cpu')))","outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"4419a3bc-bbe0-4c1b-87ac-34244b54a721"},"source":"class LineMODDataset(Dataset):\n\n    \"\"\"\n    Args:\n        root_dir (str): path to the dataset\n        classes (dictionary): values of classes to extract from segmentation mask \n        transform : Transforms for input image\n            \"\"\"\n\n    def __init__(self,root_dir,classes=None,transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.classes = classes\n        self.list_all_images = load_obj(root_dir + \"all_images_adr\")\n        self.training_images_idx = load_obj(root_dir + \"train_images_indices\")\n\n    def __len__(self):\n        return len(self.training_images_idx)\n    def __getitem__(self, i):\n        img_adr = self.list_all_images[self.training_images_idx[i]]\n        label = os.path.split(os.path.split(os.path.dirname(img_adr))[0])[1]\n        regex = re.compile(r'\\d+')\n        idx = regex.findall(os.path.split(img_adr)[1])[0]\n        image = cv2.imread(img_adr) \n        IDmask = cv2.imread(self.root_dir + label + \"/ground_truth/IDmasks/color\" + str(idx) + \".png\"\n                                ,cv2.IMREAD_GRAYSCALE)\n        Umask = cv2.imread(self.root_dir + label + \"/ground_truth/Umasks/color\" + str(idx) + \".png\"\n                                ,cv2.IMREAD_GRAYSCALE)\n        Vmask = cv2.imread(self.root_dir + label + \"/ground_truth/Vmasks/color\" + str(idx) + \".png\"\n                                ,cv2.IMREAD_GRAYSCALE)\n        # resize the masks\n        IDmask = (IDmask/255)*self.classes[label] \n        image = cv2.resize(image,(image.shape[1]//2, image.shape[0]//2), interpolation=cv2.INTER_AREA)\n        IDmask = cv2.resize(IDmask,(IDmask.shape[1]//2, IDmask.shape[0]//2), interpolation=cv2.INTER_AREA)\n        Umask = cv2.resize(Umask,(Umask.shape[1]//2, Umask.shape[0]//2), interpolation=cv2.INTER_AREA)\n        Vmask = cv2.resize(Vmask,(Vmask.shape[1]//2, Vmask.shape[0]//2), interpolation=cv2.INTER_AREA)\n        if self.transform:\n            image = self.transform(image)\n        IDmask = (torch.from_numpy(IDmask)).type(torch.int64)\n        Umask = (torch.from_numpy(Umask)).type(torch.int64)\n        Vmask = (torch.from_numpy(Vmask)).type(torch.int64)\n        return img_adr,image,IDmask,Umask,Vmask","outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"ef27f7e2-9060-43f8-992a-a5f20bee6270"},"source":"root_dir = \"/home/jovyan/work/LineMOD_Dataset/\"\ntrain_data = LineMODDataset(\n    root_dir, \n    classes = classes, \n    transform = transforms.Compose([transforms.ToTensor()])\n)","outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"0da4eebd-44ed-4f68-bd07-8dce6f62bb00"},"source":"#Upsample the image:\nupsampled = nn.Upsample(size=[240,320] ,mode='bilinear')","outputs":[]},{"cell_type":"markdown","source":"## Create rendered images","metadata":{"tags":[],"cell_id":"871bfbe7-8a10-4d61-a0e0-e95c404affd8"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"b5dd4973-79bc-4f20-81e8-6a2b55fe3e29"},"source":"fx=572.41140; px=325.26110; fy=573.57043; py=242.04899 # Intrinsic Parameters of the Camera\nintrinsic_matrix =  np.array([[fx, 0, px], [0, fy, py], [0, 0, 1]])","outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"e72a4efa-ff0f-4077-b0f1-5031cee9b73e"},"source":"data_dir = \"/home/jovyan/work/LineMOD_Dataset/\"","outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"7b749ec1-d730-4dd6-920f-d2ce1f1e9361"},"source":"def create_rendering(root_dir,obj,idx): # helper function to help with creating renderings\n    pred_pose_adr = root_dir + obj + '/predicted_pose'+ '/info_' + str(idx) +\".txt\"\n    rgb_values = np.loadtxt(root_dir + obj + '/object.xyz',skiprows=1,usecols = (6,7,8))\n    coords_3d = np.loadtxt(root_dir + obj + '/object.xyz',skiprows=1,usecols = (0,1,2))\n    ones = np.ones((coords_3d.shape[0],1))\n    homogenous_coordinate = np.append(coords_3d, ones, axis=1)\n    rigid_transformation = np.loadtxt(pred_pose_adr)\n    # Perspective Projection to obtain 2D coordinates \n    homogenous_2D = intrinsic_matrix @ (rigid_transformation @ homogenous_coordinate.T)\n    homogenous_2D[2,:][np.where(homogenous_2D[2,:] == 0)] = 1 # to ensure there is no division by 0\n    coord_2D = homogenous_2D[:2,:] / homogenous_2D[2,:]\n    coord_2D = ((np.floor(coord_2D)).T).astype(int)\n    rendered_image = np.zeros ((480,640,3))\n    x_2d = np.clip(coord_2D[:,0],0,480)\n    y_2d = np.clip(coord_2D[:,1],0,640)\n    rendered_image[x_2d,y_2d,:] = rgb_values\n    temp = np.sum(rendered_image,axis = 2)\n    non_zero_indices = np.argwhere(temp > 0)\n    min_x = non_zero_indices[:,0].min()\n    max_x = non_zero_indices[:,0].max()\n    min_y = non_zero_indices[:,1].min()\n    max_y = non_zero_indices[:,1].max()\n    cropped_rendered_image = rendered_image[min_x:max_x + 1, min_y:max_y + 1,:]\n    if cropped_rendered_image.shape[0] > 240 or cropped_rendered_image.shape[1] > 320:\n        cropped_rendered_image = cv2.resize(np.float32(cropped_rendered_image),(320,240),interpolation=cv2.INTER_AREA)\n    return cropped_rendered_image\n","outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"4b15b93f-e37d-48ff-bac4-1d5b6087ad95","output_cleared":false},"source":"regex = re.compile(r'\\d+')\n# for i in range(len(train_data)):\n#     if i % 1000 == 0:\n#         print(str(i) + \"/\" + str(len(train_data)) + \"finished!\")\nimg_adr,img,_, _ , _ = train_data[0]\nlabel = os.path.split(os.path.split(os.path.dirname(img_adr))[0])[1]\nidx = regex.findall(os.path.split(img_adr)[1])[0]\nadr_rendered = data_dir + label + \"/pose_refinement/rendered/color\" + str(idx) +\".png\"\nadr_img = data_dir + label + \"/pose_refinement/real/color\" + str(idx) +\".png\"\n# find the object in the image using the idmask\nimg = img.view(1,img.shape[0],img.shape[1],img.shape[2])\nidmask_pred,_,_ = correspondence_block(img.cuda())\nidmask = torch.argmax(idmask_pred,dim=1).squeeze().cpu() \ncoord_2d = (idmask == classes[label]).nonzero(as_tuple=True)\ncoord_2d = torch.cat((coord_2d[0].view(coord_2d[0].shape[0],1),coord_2d[1].view(coord_2d[1].shape[0],1)),1)\nmin_x = coord_2d[:,0].min()\nmax_x = coord_2d[:,0].max()\nmin_y = coord_2d[:,1].min()\nmax_y = coord_2d[:,1].max()\nimg = img.squeeze().transpose(1, 2).transpose(0,2)\nobj_img = img[min_x:max_x+1,min_y:max_y+1,:]\n#saving in the correct format using upsampling\nobj_img = obj_img.transpose(0,1).transpose(0,2).unsqueeze(dim=0)\nobj_img = upsampled(obj_img)\nobj_img = obj_img.squeeze().transpose(0,2).transpose(0,1)\nmpimg.imsave(adr_img,obj_img.squeeze().numpy())\n\n#create rendering for an image\ncropped_rendered_image = create_rendering(data_dir,label,idx)\nrendered_img = torch.from_numpy(cropped_rendered_image)\nrendered_img = rendered_img.unsqueeze(dim = 0)\nrendered_img = rendered_img.transpose(1,3).transpose(2,3)\nrendered_img = upsampled(rendered_img)\nrendered_img = rendered_img.squeeze().transpose(0,2).transpose(0,1)\nmpimg.imsave(adr_rendered,rendered_img.numpy())","outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"09ccd865-e5c7-490f-89a2-eb7cffe72444"},"source":"class PoseRefinerDataset(Dataset):\n\n    \"\"\"\n    Args:\n        root_dir (str): path to the dataset directory\n        classes (dict): dictionary containing classes as key  \n        transform : Transforms for input image\n            \"\"\"\n    def __init__(self,root_dir,classes=None,transform=None):\n        self.root_dir = root_dir \n        self.transform = transform\n        self.classes = classes\n        self.list_all_images = load_obj(root_dir + \"all_images_adr\")\n        self.training_images_idx = load_obj(root_dir + \"train_images_indices\")\n\n    def __len__(self):\n        return 2 * len(self.list_all_images)\n\n    def __getitem__(self,i):\n        img_adr = self.list_all_images[self.training_images_idx[i]]\n        label = os.path.split(os.path.split(os.path.dirname(img_adr))[0])[1]\n        regex = re.compile(r'\\d+')\n        idx = regex.findall(os.path.split(img_adr)[1])[0]\n        image = cv2.imread(self.root_dir + label + '/pose_refinement/real/color' + str(idx) + \".png\") \n        rendered = cv2.imread(self.root_dir + label + '/pose_refinement/rendered/color' + str(idx) + \".png\",cv2.IMREAD_GRAYSCALE)\n        rendered = cv2.cvtColor(rendered.astype('uint8'),cv2.COLOR_GRAY2RGB)\n        true_pose = get_rot_tra(self.root_dir + obj + '/data/rot' + str(idx) +\".rot\",\n                        self.root_dir + obj + '/data/tra' + str(idx) +\".tra\")\n        pred_pose_adr = self.root_dir + obj + '/predicted_pose/info_' + str(idx) +\".txt\"\n        pred_pose = np.loadtxt(pred_pose_adr)\n        if self.transform:\n            image = self.transform(image)\n            rendered = self.transform(rendered)\n        return label,image,rendered,true_pose,pred_pose","outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"1201bd81-7820-4336-8436-d8b85d53a024"},"source":"root_dir = \"/home/jovyan/work/LineMOD_Dataset/\"\ntrain_data = PoseRefinerDataset(\n    root_dir, \n    classes = classes, \n    transform = transforms.Compose([\n        transforms.ToPILImage(mode=None),\n        transforms.Resize(size=(224,224)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n)","outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"4d0a3914-689c-45ad-b7df-5faf3b058470","output_cleared":false},"source":"pose_refiner = Pose_Refiner()\npose_refiner.cuda()\n# freeze resnet\n# pose_refiner.feature_extractor[0].weight.requires_grad = False","outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /home/jovyan/.cache/torch/checkpoints/resnet18-5c106cde.pth\n100.0%\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"d40a394b-3bd4-4dc5-bab1-f9ee3934b8b9"},"source":"batch_size = 1\nnum_workers = 0\nvalid_size = 0.2\n# obtain training indices that will be used for validation\nnum_train = len(train_data)\nindices = list(range(num_train))\nnp.random.shuffle(indices)\nsplit = int(np.floor(valid_size * num_train))\ntrain_idx, valid_idx = indices[split:], indices[:split]\n\n# define samplers for obtaining training and validation batches\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)\n\n# prepare data loaders (combine dataset and sampler)\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n    sampler=train_sampler, num_workers=num_workers)\nvalid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n    sampler=valid_sampler, num_workers=num_workers)","outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"74715a8f-d9ec-4944-b490-732f6aea48cd"},"source":"optimizer = optim.Adam(pose_refiner.parameters(), lr=3e-2,weight_decay=3e-5)","outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"094f02b4-a392-4483-97b2-663c222d327c"},"source":"def ADD_loss(pt_cld,true_pose,pred_pose):\n    \n    index = np.random.choice(pt_cld.shape[0], 2000, replace=False)  \n    pt_cld_rand = pt_cld[index,:]\n    target = torch.tensor(pt_cld_rand) @ true_pose[0:3,0:3] + torch.tensor([true_pose[0,3], true_pose[1,3], true_pose[2,3]])\n    output = torch.tensor(pt_cld_rand) @ pred_pose[0:3,0:3] + torch.tensor([pred_pose[0,3], pred_pose[1,3], pred_pose[2,3]])\n    loss = ((output - target)**2).sum()/pt_cld_rand.shape[0]\n\n    return loss","outputs":[]},{"cell_type":"markdown","source":"## Training Loop","metadata":{"tags":[],"cell_id":"959bd886-ac47-404a-bf00-8e8d8e217e9a"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"5273ca6c-c39b-45db-94a5-b41d0d44fc79"},"source":"# number of epochs to train the model\nn_epochs = 5\n\nvalid_loss_min = np.Inf # track change in validation loss\noutliers = 0\nfor epoch in range(1, n_epochs+1):\n\n    print(\"----- Epoch Number: \",epoch,\"--------\")\n    \n    # keep track of training and validation loss\n    train_loss = 0.0\n    valid_loss = 0.0\n    \n    ###################\n    # train the model #\n    ###################\n    pose_refiner.train()\n    for j, (label,image,rendered,true_pose,pred_pose) in enumerate(train_loader):\n        if j % 1000 == 0 and j != 0:\n            print(\"Training Loss: \",train_loss/j,\" Iteration: \",j)\n        pred_pose = pred_pose.squeeze()\n        true_pose = true_pose.squeeze()\n        # move tensors to GPU \n        image, rendered = image.cuda(), rendered.cuda()\n        # clear the gradients of all optimized variables\n        optimizer.zero_grad()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        xy,z,rot = pose_refiner(image,rendered,pred_pose) \n        # convert R quarternion to rotational matrix\n        rot = torch.tensor((R.from_quat(rot.detach().cpu().numpy())).as_matrix())\n        # update predicted pose\n        pred_pose[0:3,0:3] = rot \n        pred_pose[0,3] = xy[0]\n        pred_pose[1,3] = xy[1]\n        pred_pose[2,3] = z\n        # fetch point cloud data\n        ptcld_file = root_dir  + label + \"/object.xyz\"\n        pt_cld = np.loadtxt(ptcld_file, skiprows=1, usecols = (0,1,2))\n        # calculate the batch loss\n        loss = ADD_loss(pt_cld,true_pose,pred_pose)\n        if loss.item() < 1000: #filter out the outliers\n            # backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n            # perform a single optimization step (parameter update)\n            optimizer.step()\n            # update training loss\n            train_loss += loss.item()\n        else:\n            outliers += 1 \n\n\n    ######################    \n    # validate the model #\n    ######################\n    pose_refiner.eval()\n    for label,image,rendered,true_pose,pred_pose in valid_loader:\n        pred_pose = pred_pose.squeeze()\n        true_pose = true_pose.squeeze()       \n        # move tensors to GPU if CUDA is available\n        if train_on_gpu:\n            image, rendered = image.cuda(), rendered.cuda()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        xy,z,rot = pose_refiner(image,rendered,pred_pose)\n        # convert R quarternion to rotational matrix\n        rot = torch.tensor((R.from_quat(rot.detach().cpu().numpy())).as_matrix())\n        # update predicted pose \n        pred_pose[0:3,0:3] = rot \n        pred_pose[0,3] = xy[0]\n        pred_pose[1,3] = xy[1]\n        pred_pose[2,3] = z\n        # fetch point cloud data\n        ptcld_file = root_dir  + label + \"/object.xyz\"\n        pt_cld = np.loadtxt(ptcld_file, skiprows=1, usecols = (0,1,2))\n        # calculate the batch loss\n        loss = ADD_loss(pt_cld,true_pose,pred_pose)\n        # update average validation loss \n        if loss.item() < 1000:\n            valid_loss += loss.item()\n        else:\n            outliers += 1\n    \n    # calculate average losses\n    train_loss = train_loss/len(train_loader.sampler)\n    valid_loss = valid_loss/len(valid_loader.sampler)\n        \n    # print training/validation statistics \n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n        epoch, train_loss, valid_loss))\n    \n    # save model if validation loss has decreased\n    if valid_loss <= valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,valid_loss))\n        torch.save(pose_refiner.state_dict(), 'pose_refiner.pt')\n        valid_loss_min = valid_loss\nprint(\"Number of Outliers: \",outliers)","execution_count":0,"outputs":[{"output_type":"error","ename":"KernelInterrupted","evalue":"Execution interrupted by the Jupyter kernel.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKernelInterrupted\u001b[0m: Execution interrupted by the Jupyter kernel."]}]}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_execution_queue":[],"deepnote_notebook_id":"6e66fde7-3c8a-4379-a9be-4eb7b8bf1d9f"}}