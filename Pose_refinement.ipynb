{"cells":[{"cell_type":"markdown","source":"# Create data for Deep Model-based Pose refinement","metadata":{"tags":[],"cell_id":"ec7c9573-6e5d-4076-a3aa-92cef31c1d7c"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"eb6d1d8d-d6ff-46d7-a9b1-51a10b4e914d"},"source":"!pip install opencv-python","outputs":[{"name":"stdout","text":"Collecting opencv-python\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/f0/cfe88d262c67825b20d396c778beca21829da061717c7aaa8b421ae5132e/opencv_python-4.2.0.34-cp37-cp37m-manylinux1_x86_64.whl (28.2MB)\n\u001b[K     |████████████████████████████████| 28.2MB 3.3MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /opt/venv/lib/python3.7/site-packages (from opencv-python) (1.18.1)\nInstalling collected packages: opencv-python\nSuccessfully installed opencv-python-4.2.0.34\n\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"8a076704-0b9b-419c-8b79-cc54f101d39d"},"source":"import numpy as np\nimport os\nimport re\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport cv2\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils, models\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport unet_model as UNET\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom create_ground_truth_helper import get_rot_tra\nfrom pose_refiner_block import Pose_Refiner\nfrom scipy.spatial.transform import Rotation as R\nfrom helper import *\n%load_ext autoreload\n%autoreload 2","outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"111ed625-f2b3-4f57-957a-f77c4d1969eb"},"source":"classes = {'ape':1, 'benchviseblue':2, 'bowl':3, 'can':4, 'cat':5, 'cup':6, 'driller':7, \n            'duck':8, 'glue':9, 'holepuncher':10, 'iron':11, 'lamp':12, 'phone':13, 'cam':14,'eggbox':15}","outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"9cc7f06d-353a-4a85-9b35-e576d50669fc"},"source":"correspondence_block = UNET.UNet(n_channels = 3, out_channels_id = 16, out_channels_uv = 256, bilinear=True)\ncorrespondence_block.cuda()","outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"UNet(\n  (inc): DoubleConv(\n    (double_conv): Sequential(\n      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (down1): Down(\n    (maxpool_conv): Sequential(\n      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (1): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n  )\n  (down2): Down(\n    (maxpool_conv): Sequential(\n      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (1): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n  )\n  (down3): Down(\n    (maxpool_conv): Sequential(\n      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (1): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n  )\n  (down4): Down(\n    (maxpool_conv): Sequential(\n      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (1): DoubleConv(\n        (double_conv): Sequential(\n          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n  )\n  (up1_id): Up(\n    (up): Upsample(scale_factor=2.0, mode=bilinear)\n    (conv): DoubleConv(\n      (double_conv): Sequential(\n        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (up2_id): Up(\n    (up): Upsample(scale_factor=2.0, mode=bilinear)\n    (conv): DoubleConv(\n      (double_conv): Sequential(\n        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (up3_id): Up(\n    (up): Upsample(scale_factor=2.0, mode=bilinear)\n    (conv): DoubleConv(\n      (double_conv): Sequential(\n        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (up4_id): Up(\n    (up): Upsample(scale_factor=2.0, mode=bilinear)\n    (conv): DoubleConv(\n      (double_conv): Sequential(\n        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (outc_id): OutConv(\n    (conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n  )\n  (up1_uv): Up(\n    (up): Upsample(scale_factor=2.0, mode=bilinear)\n    (conv): DoubleConv(\n      (double_conv): Sequential(\n        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (up2_uv): Up(\n    (up): Upsample(scale_factor=2.0, mode=bilinear)\n    (conv): DoubleConv(\n      (double_conv): Sequential(\n        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (outc_uv1): OutConv(\n    (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n  )\n  (outc_uv2): OutConv(\n    (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n  )\n  (outc_uv3): OutConv(\n    (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n  )\n  (outc_uv4): OutConv(\n    (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n  )\n  (up3_uv): Upsample(scale_factor=2.0, mode=bilinear)\n  (up4_uv): Upsample(scale_factor=2.0, mode=bilinear)\n)"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"6567d2e0-19a9-43d2-9b91-260743e0d28e"},"source":"correspondence_block.load_state_dict(torch.load('correspondence_block.pt',map_location=torch.device('cpu')))","outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"4419a3bc-bbe0-4c1b-87ac-34244b54a721"},"source":"class LineMODDataset(Dataset):\n\n    \"\"\"\n    Args:\n        root_dir (str): path to the dataset\n        classes (dictionary): values of classes to extract from segmentation mask \n        transform : Transforms for input image\n            \"\"\"\n\n    def __init__(self,root_dir,classes=None,transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.classes = classes\n        self.list_all_images = load_obj(root_dir + \"all_images_adr\")\n        self.training_images_idx = load_obj(root_dir + \"train_images_indices\")\n\n    def __len__(self):\n        return len(self.training_images_idx)\n    def __getitem__(self, i):\n        img_adr = self.list_all_images[self.training_images_idx[i]]\n        label = os.path.split(os.path.split(os.path.dirname(img_adr))[0])[1]\n        regex = re.compile(r'\\d+')\n        idx = regex.findall(os.path.split(img_adr)[1])[0]\n        image = cv2.imread(img_adr) \n        IDmask = cv2.imread(self.root_dir + label + \"/ground_truth/IDmasks/color\" + str(idx) + \".png\"\n                                ,cv2.IMREAD_GRAYSCALE)\n        Umask = cv2.imread(self.root_dir + label + \"/ground_truth/Umasks/color\" + str(idx) + \".png\"\n                                ,cv2.IMREAD_GRAYSCALE)\n        Vmask = cv2.imread(self.root_dir + label + \"/ground_truth/Vmasks/color\" + str(idx) + \".png\"\n                                ,cv2.IMREAD_GRAYSCALE)\n        # resize the masks\n        IDmask = (IDmask/255)*self.classes[label] \n        image = cv2.resize(image,(image.shape[1]//2, image.shape[0]//2), interpolation=cv2.INTER_AREA)\n        IDmask = cv2.resize(IDmask,(IDmask.shape[1]//2, IDmask.shape[0]//2), interpolation=cv2.INTER_AREA)\n        Umask = cv2.resize(Umask,(Umask.shape[1]//2, Umask.shape[0]//2), interpolation=cv2.INTER_AREA)\n        Vmask = cv2.resize(Vmask,(Vmask.shape[1]//2, Vmask.shape[0]//2), interpolation=cv2.INTER_AREA)\n        if self.transform:\n            image = self.transform(image)\n        IDmask = (torch.from_numpy(IDmask)).type(torch.int64)\n        Umask = (torch.from_numpy(Umask)).type(torch.int64)\n        Vmask = (torch.from_numpy(Vmask)).type(torch.int64)\n        return img_adr,image,IDmask,Umask,Vmask","outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"ef27f7e2-9060-43f8-992a-a5f20bee6270"},"source":"root_dir = \"/home/jovyan/work/LineMOD_Dataset/\"\ntrain_data = LineMODDataset(\n    root_dir, \n    classes = classes, \n    transform = transforms.Compose([transforms.ToTensor()])\n)","outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"0da4eebd-44ed-4f68-bd07-8dce6f62bb00"},"source":"#Upsample the image:\nupsampled = nn.Upsample(size=[240,320] ,mode='bilinear',align_corners=False)","outputs":[]},{"cell_type":"markdown","source":"## Create rendered images","metadata":{"tags":[],"cell_id":"871bfbe7-8a10-4d61-a0e0-e95c404affd8"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"b5dd4973-79bc-4f20-81e8-6a2b55fe3e29"},"source":"fx=572.41140; px=325.26110; fy=573.57043; py=242.04899 # Intrinsic Parameters of the Camera\nintrinsic_matrix =  np.array([[fx, 0, px], [0, fy, py], [0, 0, 1]])","outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"e72a4efa-ff0f-4077-b0f1-5031cee9b73e"},"source":"data_dir = \"/home/jovyan/work/LineMOD_Dataset/\"","outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"7b749ec1-d730-4dd6-920f-d2ce1f1e9361"},"source":"def create_rendering(root_dir,obj,idx): # helper function to help with creating renderings\n    pred_pose_adr = root_dir + obj + '/predicted_pose'+ '/info_' + str(idx) +\".txt\"\n    rgb_values = np.loadtxt(root_dir + obj + '/object.xyz',skiprows=1,usecols = (6,7,8))\n    coords_3d = np.loadtxt(root_dir + obj + '/object.xyz',skiprows=1,usecols = (0,1,2))\n    ones = np.ones((coords_3d.shape[0],1))\n    homogenous_coordinate = np.append(coords_3d, ones, axis=1)\n    rigid_transformation = np.loadtxt(pred_pose_adr)\n    # Perspective Projection to obtain 2D coordinates \n    homogenous_2D = intrinsic_matrix @ (rigid_transformation @ homogenous_coordinate.T)\n    homogenous_2D[2,:][np.where(homogenous_2D[2,:] == 0)] = 1 # to ensure there is no division by 0\n    coord_2D = homogenous_2D[:2,:] / homogenous_2D[2,:]\n    coord_2D = ((np.floor(coord_2D)).T).astype(int)\n    rendered_image = np.zeros ((480,640,3))\n    x_2d = np.clip(coord_2D[:,0],0,479)\n    y_2d = np.clip(coord_2D[:,1],0,639)\n    rendered_image[x_2d,y_2d,:] = rgb_values\n    temp = np.sum(rendered_image,axis = 2)\n    non_zero_indices = np.argwhere(temp > 0)\n    min_x = non_zero_indices[:,0].min()\n    max_x = non_zero_indices[:,0].max()\n    min_y = non_zero_indices[:,1].min()\n    max_y = non_zero_indices[:,1].max()\n    cropped_rendered_image = rendered_image[min_x:max_x + 1, min_y:max_y + 1,:]\n    if cropped_rendered_image.shape[0] > 240 or cropped_rendered_image.shape[1] > 320:\n        cropped_rendered_image = cv2.resize(np.float32(cropped_rendered_image),(320,240),interpolation=cv2.INTER_AREA)\n    return cropped_rendered_image\n","outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"4b15b93f-e37d-48ff-bac4-1d5b6087ad95","output_cleared":false},"source":"regex = re.compile(r'\\d+')\nfor i in range(len(train_data)):\n    if i % 1000 == 0:\n        print(str(i) + \"/\" + str(len(train_data)) + \" finished!\")\n    img_adr,img,_, _ , _ = train_data[i]\n    label = os.path.split(os.path.split(os.path.dirname(img_adr))[0])[1]\n    idx = regex.findall(os.path.split(img_adr)[1])[0]\n    adr_rendered = data_dir + label + \"/pose_refinement/rendered/color\" + str(idx) +\".png\"\n    adr_img = data_dir + label + \"/pose_refinement/real/color\" + str(idx) +\".png\"\n    # find the object in the image using the idmask\n    img = img.view(1,img.shape[0],img.shape[1],img.shape[2])\n    idmask_pred,_,_ = correspondence_block(img.cuda())\n    idmask = torch.argmax(idmask_pred,dim=1).squeeze().cpu() \n    coord_2d = (idmask == classes[label]).nonzero(as_tuple=True)\n    coord_2d = torch.cat((coord_2d[0].view(coord_2d[0].shape[0],1),coord_2d[1].view(coord_2d[1].shape[0],1)),1)\n    min_x = coord_2d[:,0].min()\n    max_x = coord_2d[:,0].max()\n    min_y = coord_2d[:,1].min()\n    max_y = coord_2d[:,1].max()\n    img = img.squeeze().transpose(1, 2).transpose(0,2)\n    obj_img = img[min_x:max_x+1,min_y:max_y+1,:]\n    #saving in the correct format using upsampling\n    obj_img = obj_img.transpose(0,1).transpose(0,2).unsqueeze(dim=0)\n    obj_img = upsampled(obj_img)\n    obj_img = obj_img.squeeze().transpose(0,2).transpose(0,1)\n    mpimg.imsave(adr_img,obj_img.squeeze().numpy())\n\n    #create rendering for an image\n    cropped_rendered_image = create_rendering(data_dir,label,idx)\n    rendered_img = torch.from_numpy(cropped_rendered_image)\n    rendered_img = rendered_img.unsqueeze(dim = 0)\n    rendered_img = rendered_img.transpose(1,3).transpose(2,3)\n    rendered_img = upsampled(rendered_img)\n    rendered_img = rendered_img.squeeze().transpose(0,2).transpose(0,1)\n    mpimg.imsave(adr_rendered,rendered_img.numpy())","outputs":[{"name":"stdout","text":"0/14620 finished!\n1000/14620 finished!\n2000/14620 finished!\n3000/14620 finished!\n4000/14620 finished!\n5000/14620 finished!\n6000/14620 finished!\n7000/14620 finished!\n8000/14620 finished!\n9000/14620 finished!\n10000/14620 finished!\n11000/14620 finished!\n12000/14620 finished!\n13000/14620 finished!\n14000/14620 finished!\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"09ccd865-e5c7-490f-89a2-eb7cffe72444"},"source":"class PoseRefinerDataset(Dataset):\n\n    \"\"\"\n    Args:\n        root_dir (str): path to the dataset directory\n        classes (dict): dictionary containing classes as key  \n        transform : Transforms for input image\n            \"\"\"\n    def __init__(self,root_dir,classes=None,transform=None):\n        self.root_dir = root_dir \n        self.transform = transform\n        self.classes = classes\n        self.list_all_images = load_obj(root_dir + \"all_images_adr\")\n        self.training_images_idx = load_obj(root_dir + \"train_images_indices\")\n\n    def __len__(self):\n        return 2 * len(self.training_images_idx)\n\n    def __getitem__(self,i):\n        img_adr = self.list_all_images[self.training_images_idx[i]]\n        label = os.path.split(os.path.split(os.path.dirname(img_adr))[0])[1]\n        regex = re.compile(r'\\d+')\n        idx = regex.findall(os.path.split(img_adr)[1])[0]\n        image = cv2.imread(self.root_dir + label + '/pose_refinement/real/color' + str(idx) + \".png\") \n        rendered = cv2.imread(self.root_dir + label + '/pose_refinement/rendered/color' + str(idx) + \".png\",cv2.IMREAD_GRAYSCALE)\n        rendered = cv2.cvtColor(rendered.astype('uint8'),cv2.COLOR_GRAY2RGB)\n        true_pose = get_rot_tra(self.root_dir + obj + '/data/rot' + str(idx) +\".rot\",\n                        self.root_dir + obj + '/data/tra' + str(idx) +\".tra\")\n        pred_pose_adr = self.root_dir + obj + '/predicted_pose/info_' + str(idx) +\".txt\"\n        pred_pose = np.loadtxt(pred_pose_adr)\n        if self.transform:\n            image = self.transform(image)\n            rendered = self.transform(rendered)\n        return label,image,rendered,true_pose,pred_pose","outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"1201bd81-7820-4336-8436-d8b85d53a024"},"source":"root_dir = \"/home/jovyan/work/LineMOD_Dataset/\"\ntrain_data = PoseRefinerDataset(\n    root_dir, \n    classes = classes, \n    transform = transforms.Compose([\n        transforms.ToPILImage(mode=None),\n        transforms.Resize(size=(224,224)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n)","outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"0c549966-5092-4751-a618-c5da34a8044f"},"source":"len(train_data)","outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"29240"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"4d0a3914-689c-45ad-b7df-5faf3b058470","output_cleared":false},"source":"pose_refiner = Pose_Refiner()\npose_refiner.cuda()\n# freeze resnet\n# pose_refiner.feature_extractor[0].weight.requires_grad = False","outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /home/jovyan/.cache/torch/checkpoints/resnet18-5c106cde.pth\n100.0%\n","output_type":"stream"},{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"Pose_Refiner(\n  (feature_extractor): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n  )\n  (fc_xyhead_1): Linear(in_features=512, out_features=253, bias=True)\n  (fc_xyhead_2): Linear(in_features=256, out_features=2, bias=True)\n  (fc_zhead): Sequential(\n    (0): Linear(in_features=512, out_features=256, bias=True)\n    (1): Linear(in_features=256, out_features=1, bias=True)\n  )\n  (fc_Rhead_1): Linear(in_features=512, out_features=252, bias=True)\n  (fc_Rhead_2): Linear(in_features=256, out_features=4, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"d40a394b-3bd4-4dc5-bab1-f9ee3934b8b9"},"source":"batch_size = 1\nnum_workers = 0\nvalid_size = 0.2\n# obtain training indices that will be used for validation\nnum_train = len(train_data)\nindices = list(range(num_train))\nnp.random.shuffle(indices)\nsplit = int(np.floor(valid_size * num_train))\ntrain_idx, valid_idx = indices[split:], indices[:split]\n\n# define samplers for obtaining training and validation batches\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)\n\n# prepare data loaders (combine dataset and sampler)\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n    sampler=train_sampler, num_workers=num_workers)\nvalid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n    sampler=valid_sampler, num_workers=num_workers)","outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"74715a8f-d9ec-4944-b490-732f6aea48cd"},"source":"optimizer = optim.Adam(pose_refiner.parameters(), lr=3e-4,weight_decay=3e-5)","outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"094f02b4-a392-4483-97b2-663c222d327c"},"source":"def Matching_loss(pt_cld,true_pose,pred_pose): # no. of points is always 3000 \n    \n    index = np.random.choice(pt_cld.shape[0], 3000, replace=False)  \n    pt_cld_rand = pt_cld[index,:]\n    target = torch.tensor(pt_cld_rand) @ true_pose[0:3,0:3] + torch.tensor([true_pose[0,3], true_pose[1,3], true_pose[2,3]])\n    output = torch.tensor(pt_cld_rand) @ pred_pose[0:3,0:3] + torch.tensor([pred_pose[0,3], pred_pose[1,3], pred_pose[2,3]])\n    loss = (torch.abs(output - target)).sum()/3000\n\n    return loss","outputs":[]},{"cell_type":"markdown","source":"## Training Loop","metadata":{"tags":[],"cell_id":"959bd886-ac47-404a-bf00-8e8d8e217e9a"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"5273ca6c-c39b-45db-94a5-b41d0d44fc79"},"source":"# number of epochs to train the model\nn_epochs = 5\n\nvalid_loss_min = np.Inf # track change in validation loss\noutliers = 0\nfor epoch in range(1, n_epochs+1):\n\n    print(\"----- Epoch Number: \",epoch,\"--------\")\n    \n    # keep track of training and validation loss\n    train_loss = 0.0\n    valid_loss = 0.0\n    \n    ###################\n    # train the model #\n    ###################\n    pose_refiner.train()\n    for label,image,rendered,true_pose,pred_pose in enumerate(train_loader):\n        pred_pose = pred_pose.squeeze()\n        true_pose = true_pose.squeeze()\n        # move tensors to GPU \n        image, rendered = image.cuda(), rendered.cuda()\n        # clear the gradients of all optimized variables\n        optimizer.zero_grad()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        xy,z,rot = pose_refiner(image,rendered,pred_pose) \n        # convert R quarternion to rotational matrix\n        rot = torch.tensor((R.from_quat(rot.detach().cpu().numpy())).as_matrix())\n        # update predicted pose\n        pred_pose[0:3,0:3] = rot \n        pred_pose[0,3] = xy[0]\n        pred_pose[1,3] = xy[1]\n        pred_pose[2,3] = z\n        # fetch point cloud data\n        ptcld_file = root_dir  + label + \"/object.xyz\"\n        pt_cld = np.loadtxt(ptcld_file, skiprows=1, usecols = (0,1,2))\n        # calculate the batch loss\n        loss = Matching_loss(pt_cld,true_pose,pred_pose)\n        if loss.item() < 1000: #filter out the outliers\n            # backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n            # perform a single optimization step (parameter update)\n            optimizer.step()\n            # update training loss\n            train_loss += loss.item()\n        else:\n            outliers += 1 \n\n\n    ######################    \n    # validate the model #\n    ######################\n    pose_refiner.eval()\n    for label,image,rendered,true_pose,pred_pose in valid_loader:\n        pred_pose = pred_pose.squeeze()\n        true_pose = true_pose.squeeze()       \n        # move tensors to GPU if CUDA is available\n        if train_on_gpu:\n            image, rendered = image.cuda(), rendered.cuda()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        xy,z,rot = pose_refiner(image,rendered,pred_pose)\n        # convert R quarternion to rotational matrix\n        rot = torch.tensor((R.from_quat(rot.detach().cpu().numpy())).as_matrix())\n        # update predicted pose \n        pred_pose[0:3,0:3] = rot \n        pred_pose[0,3] = xy[0]\n        pred_pose[1,3] = xy[1]\n        pred_pose[2,3] = z\n        # fetch point cloud data\n        ptcld_file = root_dir  + label + \"/object.xyz\"\n        pt_cld = np.loadtxt(ptcld_file, skiprows=1, usecols = (0,1,2))\n        # calculate the batch loss\n        loss = Matching_loss(pt_cld,true_pose,pred_pose)\n        # update average validation loss \n        if loss.item() < 1000:\n            valid_loss += loss.item()\n        else:\n            outliers += 1\n    \n    # calculate average losses\n    train_loss = train_loss/len(train_loader.sampler)\n    valid_loss = valid_loss/len(valid_loader.sampler)\n        \n    # print training/validation statistics \n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n        epoch, train_loss, valid_loss))\n    \n    # save model if validation loss has decreased\n    if valid_loss <= valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,valid_loss))\n        torch.save(pose_refiner.state_dict(), 'pose_refiner.pt')\n        valid_loss_min = valid_loss\nprint(\"Number of Outliers: \",outliers)","outputs":[{"name":"stdout","text":"----- Epoch Number:  1 --------\n","output_type":"stream"},{"output_type":"error","ename":"IndexError","evalue":"list index out of range","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-47524ff42675>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m###################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mpose_refiner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrendered\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrue_pose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_pose\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mpred_pose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_pose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtrue_pose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrue_pose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-b2ac4e45f47b>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mimg_adr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_all_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_images_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_adr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mregex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\d+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"6f1a7c4f-1891-48e9-9307-8bc8dff14ca4"},"source":"","execution_count":null,"outputs":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_execution_queue":[],"deepnote_notebook_id":"6e66fde7-3c8a-4379-a9be-4eb7b8bf1d9f"}}